/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

configurations {
    integrationImplementation.extendsFrom testImplementation
    integrationRuntime.extendsFrom testRuntimeOnly
}

String scalaVersion = System.getProperty("scalaVersion") != null ? System.getProperty("scalaVersion") : System.getProperty("defaultScalaVersion")
String sparkVersionsString = System.getProperty("sparkVersions") != null ? System.getProperty("sparkVersions") : System.getProperty("defaultSparkVersions")
List<String> sparkVersions = sparkVersionsString != null && !sparkVersionsString.isEmpty() ? sparkVersionsString.split(",") : []

dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')
    implementation project(':iceberg-parquet')
    implementation "com.fasterxml.jackson.core:jackson-databind"
    annotationProcessor "org.immutables:value"
    compileOnly "org.immutables:value"

    compileOnly "io.delta:delta-standalone_${scalaVersion}"

    compileOnly("org.apache.hadoop:hadoop-common") {
        exclude group: 'org.apache.avro', module: 'avro'
        exclude group: 'org.slf4j', module: 'slf4j-log4j12'
        exclude group: 'javax.servlet', module: 'servlet-api'
        exclude group: 'com.google.code.gson', module: 'gson'
    }

    // The newest version of delta-core uses Spark 3.3.*. Since its only for test, we do
    // not need to include older version of delta-core
    if (sparkVersions.contains("3.3")) {
        integrationImplementation "io.delta:delta-core_${scalaVersion}"
        integrationImplementation project(path: ":iceberg-spark:iceberg-spark-3.3_${scalaVersion}")
        integrationImplementation("org.apache.hadoop:hadoop-minicluster") {
            exclude group: 'org.apache.avro', module: 'avro'
            // to make sure netty libs only come from project(':iceberg-arrow')
            exclude group: 'io.netty', module: 'netty-buffer'
            exclude group: 'io.netty', module: 'netty-common'
        }
        integrationImplementation project(path: ':iceberg-hive-metastore')
        integrationImplementation project(path: ':iceberg-hive-metastore', configuration: 'testArtifacts')
        integrationImplementation("org.apache.spark:spark-hive_${scalaVersion}:3.3.2") {
            exclude group: 'org.apache.avro', module: 'avro'
            exclude group: 'org.apache.arrow'
            exclude group: 'org.apache.parquet'
            // to make sure netty libs only come from project(':iceberg-arrow')
            exclude group: 'io.netty', module: 'netty-buffer'
            exclude group: 'io.netty', module: 'netty-common'
            exclude group: 'org.roaringbitmap'
        }
    }
}

// The newest version of delta-core uses Spark 3.3.*. The integration test should only be built
// if iceberg-spark-3.3 is available
if (sparkVersions.contains("3.3")) {
    sourceSets {
        integration {
            java.srcDir "$projectDir/src/integration/java"
            resources.srcDir "$projectDir/src/integration/resources"
            compileClasspath += main.output + test.output
            runtimeClasspath += main.output + test.output
        }
    }

    task integrationTest(type: Test) {
        testClassesDirs = sourceSets.integration.output.classesDirs
        classpath = sourceSets.integration.runtimeClasspath
        jvmArgs += project.property('extraJvmArgs')
    }
    check.dependsOn integrationTest
}

// use integration test since we can take advantages of spark 3.3 to read datafiles of delta lake table
// and create some tests involving sql query.
test {
    useJUnitPlatform()
}
