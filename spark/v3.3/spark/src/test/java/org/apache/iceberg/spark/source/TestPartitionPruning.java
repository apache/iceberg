/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
package org.apache.iceberg.spark.source;

import java.io.File;
import java.io.IOException;
import java.net.URI;
import java.sql.Timestamp;
import java.time.Instant;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.function.Function;
import java.util.function.Predicate;
import java.util.stream.Collectors;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.RawLocalFileSystem;
import org.apache.iceberg.PartitionSpec;
import org.apache.iceberg.Schema;
import org.apache.iceberg.Table;
import org.apache.iceberg.TableProperties;
import org.apache.iceberg.expressions.Literal;
import org.apache.iceberg.hadoop.HadoopTables;
import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;
import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;
import org.apache.iceberg.relocated.com.google.common.collect.Maps;
import org.apache.iceberg.relocated.com.google.common.collect.Sets;
import org.apache.iceberg.spark.SparkReadOptions;
import org.apache.iceberg.spark.SparkSchemaUtil;
import org.apache.iceberg.transforms.Transforms;
import org.apache.iceberg.types.Types;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.catalyst.InternalRow;
import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.unsafe.types.UTF8String;
import org.junit.AfterClass;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;
import org.junit.runner.RunWith;
import org.junit.runners.Parameterized;

@RunWith(Parameterized.class)
public class TestPartitionPruning {

  private static final Configuration CONF = new Configuration();
  private static final HadoopTables TABLES = new HadoopTables(CONF);

  @Parameterized.Parameters(name = "format = {0}, vectorized = {1}")
  public static Object[][] parameters() {
    return new Object[][] {
      {"parquet", false},
      {"parquet", true},
      {"avro", false},
      {"orc", false},
      {"orc", true}
    };
  }

  private final String format;
  private final boolean vectorized;

  public TestPartitionPruning(String format, boolean vectorized) {
    this.format = format;
    this.vectorized = vectorized;
  }

  private static SparkSession spark = null;
  private static JavaSparkContext sparkContext = null;

  private static final Function<Object, Integer> BUCKET_FUNC =
      Transforms.bucket(3).bind(Types.IntegerType.get());
  private static final Function<Object, Object> TRUNCATE_FUNC =
      Transforms.truncate(5).bind(Types.StringType.get());
  private static final Function<Object, Integer> HOUR_FUNC =
      Transforms.hour().bind(Types.TimestampType.withoutZone());

  @BeforeClass
  public static void startSpark() {
    TestPartitionPruning.spark = SparkSession.builder().master("local[2]").getOrCreate();
    TestPartitionPruning.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());

    String optionKey = String.format("fs.%s.impl", CountOpenLocalFileSystem.scheme);
    CONF.set(optionKey, CountOpenLocalFileSystem.class.getName());
    spark.conf().set(optionKey, CountOpenLocalFileSystem.class.getName());
    spark.conf().set("spark.sql.session.timeZone", "UTC");
    spark.udf().register("bucket3", (Integer num) -> BUCKET_FUNC.apply(num), DataTypes.IntegerType);
    spark
        .udf()
        .register("truncate5", (String str) -> TRUNCATE_FUNC.apply(str), DataTypes.StringType);
    // NOTE: date transforms take the type long, not Timestamp
    spark
        .udf()
        .register(
            "hour",
            (Timestamp ts) ->
                HOUR_FUNC.apply(
                    org.apache.spark.sql.catalyst.util.DateTimeUtils.fromJavaTimestamp(ts)),
            DataTypes.IntegerType);
  }

  @AfterClass
  public static void stopSpark() {
    SparkSession currentSpark = TestPartitionPruning.spark;
    TestPartitionPruning.spark = null;
    currentSpark.stop();
  }

  private static final Schema LOG_SCHEMA =
      new Schema(
          Types.NestedField.optional(1, "id", Types.IntegerType.get()),
          Types.NestedField.optional(2, "date", Types.StringType.get()),
          Types.NestedField.optional(3, "level", Types.StringType.get()),
          Types.NestedField.optional(4, "message", Types.StringType.get()),
          Types.NestedField.optional(5, "timestamp", Types.TimestampType.withZone()));

  private static final List<LogMessage> LOGS =
      ImmutableList.of(
          LogMessage.debug("2020-02-02", "debug event 1", getInstant("2020-02-02T00:00:00")),
          LogMessage.info("2020-02-02", "info event 1", getInstant("2020-02-02T01:00:00")),
          LogMessage.debug("2020-02-02", "debug event 2", getInstant("2020-02-02T02:00:00")),
          LogMessage.info("2020-02-03", "info event 2", getInstant("2020-02-03T00:00:00")),
          LogMessage.debug("2020-02-03", "debug event 3", getInstant("2020-02-03T01:00:00")),
          LogMessage.info("2020-02-03", "info event 3", getInstant("2020-02-03T02:00:00")),
          LogMessage.error("2020-02-03", "error event 1", getInstant("2020-02-03T03:00:00")),
          LogMessage.debug("2020-02-04", "debug event 4", getInstant("2020-02-04T01:00:00")),
          LogMessage.warn("2020-02-04", "warn event 1", getInstant("2020-02-04T02:00:00")),
          LogMessage.debug("2020-02-04", "debug event 5", getInstant("2020-02-04T03:00:00")));

  private static Instant getInstant(String timestampWithoutZone) {
    Long epochMicros =
        (Long) Literal.of(timestampWithoutZone).to(Types.TimestampType.withoutZone()).value();
    return Instant.ofEpochMilli(TimeUnit.MICROSECONDS.toMillis(epochMicros));
  }

  @Rule public TemporaryFolder temp = new TemporaryFolder();

  private PartitionSpec spec =
      PartitionSpec.builderFor(LOG_SCHEMA)
          .identity("date")
          .identity("level")
          .bucket("id", 3)
          .truncate("message", 5)
          .hour("timestamp")
          .build();

  @Test
  public void testPartitionPruningIdentityString() {
    String filterCond = "date >= '2020-02-03' AND level = 'DEBUG'";
    Predicate<Row> partCondition =
        (Row r) -> {
          String date = r.getString(0);
          String level = r.getString(1);
          return date.compareTo("2020-02-03") >= 0 && level.equals("DEBUG");
        };

    runTest(filterCond, partCondition);
  }

  @Test
  public void testPartitionPruningBucketingInteger() {
    final int[] ids = new int[] {LOGS.get(3).getId(), LOGS.get(7).getId()};
    String condForIds =
        Arrays.stream(ids).mapToObj(String::valueOf).collect(Collectors.joining(",", "(", ")"));
    String filterCond = "id in " + condForIds;
    Predicate<Row> partCondition =
        (Row r) -> {
          int bucketId = r.getInt(2);
          Set<Integer> buckets =
              Arrays.stream(ids).map(BUCKET_FUNC::apply).boxed().collect(Collectors.toSet());
          return buckets.contains(bucketId);
        };

    runTest(filterCond, partCondition);
  }

  @Test
  public void testPartitionPruningTruncatedString() {
    String filterCond = "message like 'info event%'";
    Predicate<Row> partCondition =
        (Row r) -> {
          String truncatedMessage = r.getString(3);
          return truncatedMessage.equals("info ");
        };

    runTest(filterCond, partCondition);
  }

  @Test
  public void testPartitionPruningTruncatedStringComparingValueShorterThanPartitionValue() {
    String filterCond = "message like 'inf%'";
    Predicate<Row> partCondition =
        (Row r) -> {
          String truncatedMessage = r.getString(3);
          return truncatedMessage.startsWith("inf");
        };

    runTest(filterCond, partCondition);
  }

  @Test
  public void testPartitionPruningHourlyPartition() {
    String filterCond;
    if (spark.version().startsWith("2")) {
      // Looks like from Spark 2 we need to compare timestamp with timestamp to push down the
      // filter.
      filterCond = "timestamp >= to_timestamp('2020-02-03T01:00:00')";
    } else {
      filterCond = "timestamp >= '2020-02-03T01:00:00'";
    }
    Predicate<Row> partCondition =
        (Row r) -> {
          int hourValue = r.getInt(4);
          Instant instant = getInstant("2020-02-03T01:00:00");
          Integer hourValueToFilter =
              HOUR_FUNC.apply(TimeUnit.MILLISECONDS.toMicros(instant.toEpochMilli()));
          return hourValue >= hourValueToFilter;
        };

    runTest(filterCond, partCondition);
  }

  private void runTest(String filterCond, Predicate<Row> partCondition) {
    File originTableLocation = createTempDir();
    Assert.assertTrue("Temp folder should exist", originTableLocation.exists());

    Table table = createTable(originTableLocation);
    Dataset<Row> logs = createTestDataset();
    saveTestDatasetToTable(logs, table);

    List<Row> expected =
        logs.select("id", "date", "level", "message", "timestamp")
            .filter(filterCond)
            .orderBy("id")
            .collectAsList();
    Assert.assertFalse("Expected rows should be not empty", expected.isEmpty());

    // remove records which may be recorded during storing to table
    CountOpenLocalFileSystem.resetRecordsInPathPrefix(originTableLocation.getAbsolutePath());

    List<Row> actual =
        spark
            .read()
            .format("iceberg")
            .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))
            .load(table.location())
            .select("id", "date", "level", "message", "timestamp")
            .filter(filterCond)
            .orderBy("id")
            .collectAsList();
    Assert.assertFalse("Actual rows should not be empty", actual.isEmpty());

    Assert.assertEquals("Rows should match", expected, actual);

    assertAccessOnDataFiles(originTableLocation, table, partCondition);
  }

  private File createTempDir() {
    try {
      return temp.newFolder();
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }

  private Table createTable(File originTableLocation) {
    String trackedTableLocation = CountOpenLocalFileSystem.convertPath(originTableLocation);
    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);
    return TABLES.create(LOG_SCHEMA, spec, properties, trackedTableLocation);
  }

  private Dataset<Row> createTestDataset() {
    List<InternalRow> rows =
        LOGS.stream()
            .map(
                logMessage -> {
                  Object[] underlying =
                      new Object[] {
                        logMessage.getId(),
                        UTF8String.fromString(logMessage.getDate()),
                        UTF8String.fromString(logMessage.getLevel()),
                        UTF8String.fromString(logMessage.getMessage()),
                        // discard the nanoseconds part to simplify
                        TimeUnit.MILLISECONDS.toMicros(logMessage.getTimestamp().toEpochMilli())
                      };
                  return new GenericInternalRow(underlying);
                })
            .collect(Collectors.toList());

    JavaRDD<InternalRow> rdd = sparkContext.parallelize(rows);
    Dataset<Row> df =
        spark.internalCreateDataFrame(
            JavaRDD.toRDD(rdd), SparkSchemaUtil.convert(LOG_SCHEMA), false);

    return df.selectExpr("id", "date", "level", "message", "timestamp")
        .selectExpr(
            "id",
            "date",
            "level",
            "message",
            "timestamp",
            "bucket3(id) AS bucket_id",
            "truncate5(message) AS truncated_message",
            "hour(timestamp) AS ts_hour");
  }

  private void saveTestDatasetToTable(Dataset<Row> logs, Table table) {
    logs.orderBy("date", "level", "bucket_id", "truncated_message", "ts_hour")
        .select("id", "date", "level", "message", "timestamp")
        .write()
        .format("iceberg")
        .mode("append")
        .save(table.location());
  }

  private void assertAccessOnDataFiles(
      File originTableLocation, Table table, Predicate<Row> partCondition) {
    // only use files in current table location to avoid side-effects on concurrent test runs
    Set<String> readFilesInQuery =
        CountOpenLocalFileSystem.pathToNumOpenCalled.keySet().stream()
            .filter(path -> path.startsWith(originTableLocation.getAbsolutePath()))
            .collect(Collectors.toSet());

    List<Row> files =
        spark.read().format("iceberg").load(table.location() + "#files").collectAsList();

    Set<String> filesToRead = extractFilePathsMatchingConditionOnPartition(files, partCondition);
    Set<String> filesToNotRead = extractFilePathsNotIn(files, filesToRead);

    // Just to be sure, they should be mutually exclusive.
    Assert.assertTrue(Sets.intersection(filesToRead, filesToNotRead).isEmpty());

    Assert.assertFalse("The query should prune some data files.", filesToNotRead.isEmpty());

    // We don't check "all" data files bound to the condition are being read, as data files can be
    // pruned on
    // other conditions like lower/upper bound of columns.
    Assert.assertFalse(
        "Some of data files in partition range should be read. "
            + "Read files in query: "
            + readFilesInQuery
            + " / data files in partition range: "
            + filesToRead,
        Sets.intersection(filesToRead, readFilesInQuery).isEmpty());

    // Data files which aren't bound to the condition shouldn't be read.
    Assert.assertTrue(
        "Data files outside of partition range should not be read. "
            + "Read files in query: "
            + readFilesInQuery
            + " / data files outside of partition range: "
            + filesToNotRead,
        Sets.intersection(filesToNotRead, readFilesInQuery).isEmpty());
  }

  private Set<String> extractFilePathsMatchingConditionOnPartition(
      List<Row> files, Predicate<Row> condition) {
    // idx 1: file_path, idx 3: partition
    return files.stream()
        .filter(
            r -> {
              Row partition = r.getStruct(4);
              return condition.test(partition);
            })
        .map(r -> CountOpenLocalFileSystem.stripScheme(r.getString(1)))
        .collect(Collectors.toSet());
  }

  private Set<String> extractFilePathsNotIn(List<Row> files, Set<String> filePaths) {
    Set<String> allFilePaths =
        files.stream()
            .map(r -> CountOpenLocalFileSystem.stripScheme(r.getString(1)))
            .collect(Collectors.toSet());
    return Sets.newHashSet(Sets.symmetricDifference(allFilePaths, filePaths));
  }

  public static class CountOpenLocalFileSystem extends RawLocalFileSystem {
    public static String scheme =
        String.format("TestIdentityPartitionData%dfs", new Random().nextInt());
    public static Map<String, Long> pathToNumOpenCalled = Maps.newConcurrentMap();

    public static String convertPath(String absPath) {
      return scheme + "://" + absPath;
    }

    public static String convertPath(File file) {
      return convertPath(file.getAbsolutePath());
    }

    public static String stripScheme(String pathWithScheme) {
      if (!pathWithScheme.startsWith(scheme + ":")) {
        throw new IllegalArgumentException("Received unexpected path: " + pathWithScheme);
      }

      int idxToCut = scheme.length() + 1;
      while (pathWithScheme.charAt(idxToCut) == '/') {
        idxToCut++;
      }

      // leave the last '/'
      idxToCut--;

      return pathWithScheme.substring(idxToCut);
    }

    public static void resetRecordsInPathPrefix(String pathPrefix) {
      pathToNumOpenCalled.keySet().stream()
          .filter(p -> p.startsWith(pathPrefix))
          .forEach(key -> pathToNumOpenCalled.remove(key));
    }

    @Override
    public URI getUri() {
      return URI.create(scheme + ":///");
    }

    @Override
    public String getScheme() {
      return scheme;
    }

    @Override
    public FSDataInputStream open(Path f, int bufferSize) throws IOException {
      String path = f.toUri().getPath();
      pathToNumOpenCalled.compute(
          path,
          (ignored, v) -> {
            if (v == null) {
              return 1L;
            } else {
              return v + 1;
            }
          });
      return super.open(f, bufferSize);
    }
  }
}
