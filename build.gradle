/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

import groovy.transform.Memoized
import java.util.regex.Matcher
import java.util.regex.Pattern

buildscript {
  repositories {
    gradlePluginPortal()
  }
  dependencies {
    classpath 'gradle.plugin.com.github.jengelman.gradle.plugins:shadow:7.0.0'
    classpath 'com.palantir.baseline:gradle-baseline-java:4.42.0'
    // com.palantir.baseline:gradle-baseline-java:4.42.0 (the last version supporting Java 8) pulls
    // in an old version of the errorprone, which doesn't work w/ Gradle 8, so bump errorpone as
    // well.
    classpath "net.ltgt.gradle:gradle-errorprone-plugin:3.0.1"

    classpath 'com.diffplug.spotless:spotless-plugin-gradle:6.13.0'
    classpath 'gradle.plugin.org.inferred:gradle-processors:3.7.0'
    classpath 'me.champeau.jmh:jmh-gradle-plugin:0.7.1'
    classpath 'gradle.plugin.io.morethan.jmhreport:gradle-jmh-report:0.9.0'
    classpath "com.github.alisiikh:gradle-scalastyle-plugin:3.4.1"
    classpath 'com.palantir.gradle.revapi:gradle-revapi:1.7.0'
    classpath 'com.gorylenko.gradle-git-properties:gradle-git-properties:2.4.1'
    classpath 'com.palantir.gradle.gitversion:gradle-git-version:1.0.0'
    classpath ('org.eclipse.jgit:org.eclipse.jgit') {
      // gradle-git-version automatically uses a jgit version that requires JDK11
      // so we need to enforce the latest jgit version that works with JDK8
      version {
        strictly('5.13.1.202206130422-r')
      }
    }
  }
}

plugins {
  id 'nebula.dependency-recommender' version '11.0.0'
}

String scalaVersion = System.getProperty("scalaVersion") != null ? System.getProperty("scalaVersion") : System.getProperty("defaultScalaVersion")
String sparkVersionsString = System.getProperty("sparkVersions") != null ? System.getProperty("sparkVersions") : System.getProperty("defaultSparkVersions")
List<String> sparkVersions = sparkVersionsString != null && !sparkVersionsString.isEmpty() ? sparkVersionsString.split(",") : []

try {
  // apply these plugins in a try-catch block so that we can handle cases without .git directory
  apply plugin: 'com.palantir.git-version'
} catch (Exception e) {
  project.logger.error(e.getMessage())
}

if (JavaVersion.current() == JavaVersion.VERSION_1_8) {
  project.ext.jdkVersion = '8'
  project.ext.extraJvmArgs = []
} else if (JavaVersion.current() == JavaVersion.VERSION_11) {
  project.ext.jdkVersion = '11'
  project.ext.extraJvmArgs = []
} else if (JavaVersion.current() == JavaVersion.VERSION_17) {
  project.ext.jdkVersion = '17'
  project.ext.extraJvmArgs = ["-XX:+IgnoreUnrecognizedVMOptions",
                              "--add-opens", "java.base/java.io=ALL-UNNAMED",
                              "--add-opens", "java.base/java.lang.invoke=ALL-UNNAMED",
                              "--add-opens", "java.base/java.lang.reflect=ALL-UNNAMED",
                              "--add-opens", "java.base/java.lang=ALL-UNNAMED",
                              "--add-opens", "java.base/java.math=ALL-UNNAMED",
                              "--add-opens", "java.base/java.net=ALL-UNNAMED",
                              "--add-opens", "java.base/java.nio=ALL-UNNAMED",
                              "--add-opens", "java.base/java.text=ALL-UNNAMED",
                              "--add-opens", "java.base/java.time=ALL-UNNAMED",
                              "--add-opens", "java.base/java.util.concurrent.atomic=ALL-UNNAMED",
                              "--add-opens", "java.base/java.util.concurrent=ALL-UNNAMED",
                              "--add-opens", "java.base/java.util.regex=ALL-UNNAMED",
                              "--add-opens", "java.base/java.util=ALL-UNNAMED",
                              "--add-opens", "java.base/jdk.internal.ref=ALL-UNNAMED",
                              "--add-opens", "java.base/jdk.internal.reflect=ALL-UNNAMED",
                              "--add-opens", "java.sql/java.sql=ALL-UNNAMED",
                              "--add-opens", "java.base/sun.util.calendar=ALL-UNNAMED",
                              "--add-opens", "java.base/sun.nio.ch=ALL-UNNAMED",
                              "--add-opens", "java.base/sun.nio.cs=ALL-UNNAMED",
                              "--add-opens", "java.base/sun.security.action=ALL-UNNAMED",
                              "--add-opens", "java.base/sun.util.calendar=ALL-UNNAMED"]
} else {
  throw new GradleException("This build must be run with JDK 8 or 11 or 17 but was executed with JDK " + JavaVersion.current())
}

apply plugin: 'com.gorylenko.gradle-git-properties'
// git properties file for the root project for adding to the source tarball
gitProperties {
  gitPropertiesName = 'iceberg-build.properties'
  gitPropertiesResourceDir = file("${rootDir}/build")
  extProperty = 'gitProps'
  failOnNoGitDirectory = true
  keys = ['git.branch', 'git.build.version', 'git.closest.tag.name','git.commit.id.abbrev', 'git.commit.id',
          'git.commit.message.short', 'git.commit.time', 'git.tags']
}
generateGitProperties.outputs.upToDateWhen { false }

if (file("${rootDir}/iceberg-build.properties").exists()) {
  tasks.register('buildInfo', Exec) {
    project.logger.info('Using build info from iceberg-build.properties')
    commandLine 'cp', "${rootDir}/iceberg-build.properties", 'build/iceberg-build.properties'
  }
} else {
  tasks.register('buildInfo') {
    project.logger.info('Generating iceberg-build.properties from git')
    dependsOn generateGitProperties
  }
}

dependencyRecommendations {
  propertiesFile file: file('versions.props')
}

def projectVersion = getProjectVersion()
final REVAPI_PROJECTS = ["iceberg-api", "iceberg-core", "iceberg-parquet", "iceberg-orc", "iceberg-common", "iceberg-data"]

allprojects {
  group = "org.apache.iceberg"
  version = projectVersion
  repositories {
    mavenCentral()
    mavenLocal()
  }
}

subprojects {
  apply plugin: 'nebula.dependency-recommender'
  apply plugin: 'java-library'

  if (project.name in REVAPI_PROJECTS) {
    apply plugin: 'com.palantir.revapi'
    revapi {
      oldGroup = project.group
      oldName = project.name
      oldVersion = "1.2.0"
    }

    tasks.register('showDeprecationRulesOnRevApiFailure') {
      doLast {
        throw new RuntimeException("==================================================================================" +
                "\nAPI/ABI breaks detected.\n" +
                "Adding RevAPI breaks should only be done after going through a deprecation cycle." +
                "\nPlease make sure to follow the deprecation rules defined in\n" +
                "https://github.com/apache/iceberg/blob/master/CONTRIBUTING.md#semantic-versioning.\n" +
                "==================================================================================")
      }
      onlyIf {
        tasks.revapi.state.failure != null
      }
    }

    tasks.configureEach { rootTask ->
      if (rootTask.name == 'revapi') {
        rootTask.finalizedBy showDeprecationRulesOnRevApiFailure
      }
    }
  }

  configurations {
    testImplementation.extendsFrom compileOnly

    compileClasspath {
      // do not exclude Guava so the bundle project can reference classes.
      if (project.name != 'iceberg-bundled-guava') {
        exclude group: 'com.google.guava', module: 'guava'
      }
      // contains a copy of Guava
      exclude group: 'org.apache.spark', module: 'spark-network-common_2.12'
    }

    all {
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'org.mortbay.jetty'
      exclude group: 'com.sun.jersey'
      exclude group: 'com.sun.jersey.contribs'
      exclude group: 'org.pentaho', module: 'pentaho-aggdesigner-algorithm'
    }

    testArtifacts
  }

  compileJava {
    options.encoding = "UTF-8"
  }

  compileTestJava {
    options.encoding = "UTF-8"
  }

  javadoc {
    options.encoding = 'UTF-8'
  }

  sourceCompatibility = '1.8'
  targetCompatibility = '1.8'

  dependencies {
    implementation 'org.slf4j:slf4j-api'

    testImplementation 'org.junit.vintage:junit-vintage-engine'
    testImplementation 'org.junit.jupiter:junit-jupiter-engine'
    testImplementation 'org.junit.jupiter:junit-jupiter'
    testImplementation 'org.slf4j:slf4j-simple'
    testImplementation 'org.mockito:mockito-core'
    testImplementation 'org.mockito:mockito-inline'
    testImplementation 'org.assertj:assertj-core'
  }

  test {
    def logDir = "${rootDir}/build/testlogs"
    def logFile = "${logDir}/${project.name}.log"
    mkdir("${logDir}")
    delete("${logFile}")
    def buildLog = new File(logFile)
    addTestOutputListener(new TestOutputListener() {
      def lastDescriptor
      @Override
      void onOutput(TestDescriptor testDescriptor, TestOutputEvent testOutputEvent) {
        if (lastDescriptor != testDescriptor) {
          buildLog << "--------\n- Test log for: "<< testDescriptor << "\n--------\n"
          lastDescriptor = testDescriptor
        }
        buildLog << testOutputEvent.destination << " " << testOutputEvent.message
      }
    })

    maxHeapSize = "1500m"

    jvmArgs += project.property('extraJvmArgs')

    testLogging {
      events "failed"
      exceptionFormat "full"
    }
  }

  plugins.withType(ScalaPlugin.class) {
    tasks.withType(ScalaCompile.class) {
      scalaCompileOptions.keepAliveMode.set(KeepAliveMode.DAEMON)
    }
  }
}

project(':iceberg-bundled-guava') {
  apply plugin: 'com.github.johnrengelman.shadow'

  tasks.jar.dependsOn tasks.shadowJar

  dependencies {
    compileOnly('com.google.guava:guava') {
      exclude group: 'com.google.code.findbugs'
      // may be LGPL - use ALv2 findbugs-annotations instead
      exclude group: 'com.google.errorprone'
      exclude group: 'com.google.j2objc'
    }
  }

  shadowJar {
    archiveClassifier.set(null)
    configurations = [project.configurations.compileClasspath]
    zip64 true

    // include the LICENSE and NOTICE files for the shaded Jar
    from(projectDir) {
      include 'LICENSE'
      include 'NOTICE'
    }

    dependencies {
      exclude(dependency('org.slf4j:slf4j-api'))
      exclude(dependency('org.checkerframework:checker-qual'))
    }

    relocate 'com.google.common', 'org.apache.iceberg.relocated.com.google.common'

    minimize()
  }

  jar {
    archiveClassifier.set('empty')
  }
}

project(':iceberg-api') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    compileOnly "com.google.errorprone:error_prone_annotations"
    compileOnly "com.google.code.findbugs:jsr305"
    annotationProcessor "org.immutables:value"
    compileOnly "org.immutables:value"
    testImplementation "org.apache.avro:avro"
    testImplementation "com.esotericsoftware:kryo"
  }

  tasks.processTestResources.dependsOn rootProject.tasks.buildInfo

  // Workaround to prevent:
  // Task ':iceberg-api:processTestResources' uses this output of task
  // ':spotlessInternalRegisterDependencies' without declaring an explicit or implicit dependency.
  // This can lead to incorrect results being produced, depending on what order the tasks are executed.
  rootProject.tasks.configureEach { rootTask ->
    if (rootTask.name == 'spotlessInternalRegisterDependencies') {
      tasks.processTestResources.dependsOn rootTask
    }
  }

  sourceSets {
    test {
      resources {
        srcDir "${rootDir}/build"
      }
    }
  }
}

project(':iceberg-common') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
  }
}

project(':iceberg-core') {
  test {
    useJUnitPlatform()
  }
  dependencies {
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    annotationProcessor "org.immutables:value"
    compileOnly "org.immutables:value"

    implementation("org.apache.avro:avro") {
      exclude group: 'org.tukaani' // xz compression is not supported
    }

    implementation 'io.airlift:aircompressor'
    implementation 'org.apache.httpcomponents.client5:httpclient5'
    implementation "com.fasterxml.jackson.core:jackson-databind"
    implementation "com.fasterxml.jackson.core:jackson-core"
    implementation "com.github.ben-manes.caffeine:caffeine"
    implementation "org.roaringbitmap:RoaringBitmap"
    compileOnly("org.apache.hadoop:hadoop-client") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
    }

    testImplementation "org.eclipse.jetty:jetty-servlet"
    testImplementation "org.eclipse.jetty:jetty-server"
    testImplementation 'org.mock-server:mockserver-netty'
    testImplementation 'org.mock-server:mockserver-client-java'
    testImplementation "org.xerial:sqlite-jdbc"
    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation "com.esotericsoftware:kryo"
    testImplementation "com.google.guava:guava-testlib"
    testImplementation 'org.awaitility:awaitility'
  }
}

project(':iceberg-data') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-core')
    compileOnly project(':iceberg-parquet')
    compileOnly project(':iceberg-orc')
    compileOnly("org.apache.hadoop:hadoop-common") {
      exclude group: 'commons-beanutils'
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
    }

    implementation("org.apache.orc:orc-core::nohive") {
      exclude group: 'org.apache.hadoop'
      exclude group: 'commons-lang'
      // These artifacts are shaded and included in the orc-core fat jar
      exclude group: 'com.google.protobuf', module: 'protobuf-java'
      exclude group: 'org.apache.hive', module: 'hive-storage-api'
    }

    implementation("org.apache.parquet:parquet-avro") {
      exclude group: 'org.apache.avro', module: 'avro'
      // already shaded by Parquet
      exclude group: 'it.unimi.dsi'
      exclude group: 'org.codehaus.jackson'
    }

    compileOnly "org.apache.avro:avro"

    testImplementation("org.apache.hadoop:hadoop-client") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
    }

    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')
  }

  test {
    // Only for TestSplitScan as of Gradle 5.0+
    maxHeapSize '1500m'
  }
}

project(':iceberg-aliyun') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-core')
    implementation project(':iceberg-common')

    compileOnly 'com.aliyun.oss:aliyun-sdk-oss'
    compileOnly 'javax.xml.bind:jaxb-api'
    compileOnly 'javax.activation:activation'
    compileOnly 'org.glassfish.jaxb:jaxb-runtime'
    compileOnly("org.apache.hadoop:hadoop-common") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'javax.servlet', module: 'servlet-api'
      exclude group: 'com.google.code.gson', module: 'gson'
    }

    testImplementation 'com.fasterxml.jackson.dataformat:jackson-dataformat-xml'
    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation 'org.springframework:spring-web'
    testImplementation('org.springframework.boot:spring-boot-starter-jetty') {
      exclude module: 'logback-classic'
      exclude group: 'org.eclipse.jetty.websocket', module: 'javax-websocket-server-impl'
      exclude group: 'org.eclipse.jetty.websocket', module: 'websocket-server'
    }
    testImplementation('org.springframework.boot:spring-boot-starter-web') {
      exclude module: 'logback-classic'
      exclude module: 'spring-boot-starter-logging'
    }
  }
}

project(':iceberg-aws') {
  test {
    useJUnitPlatform()
  }
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')
    annotationProcessor "org.immutables:value"
    compileOnly "org.immutables:value"
    implementation "com.github.ben-manes.caffeine:caffeine"
    implementation "com.fasterxml.jackson.core:jackson-databind"
    implementation "com.fasterxml.jackson.core:jackson-core"

    compileOnly 'software.amazon.awssdk:url-connection-client'
    compileOnly 'software.amazon.awssdk:apache-client'
    compileOnly 'software.amazon.awssdk:auth'
    compileOnly 'software.amazon.awssdk:s3'
    compileOnly 'software.amazon.awssdk:kms'
    compileOnly 'software.amazon.awssdk:glue'
    compileOnly 'software.amazon.awssdk:sts'
    compileOnly 'software.amazon.awssdk:dynamodb'
    compileOnly 'software.amazon.awssdk:lakeformation'

    compileOnly("org.apache.hadoop:hadoop-common") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'javax.servlet', module: 'servlet-api'
      exclude group: 'com.google.code.gson', module: 'gson'
    }

    compileOnly 'org.apache.httpcomponents.client5:httpclient5'

    testImplementation 'software.amazon.awssdk:iam'
    testImplementation 'software.amazon.awssdk:s3control'
    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation("com.adobe.testing:s3mock-junit4") {
      exclude module: "spring-boot-starter-logging"
      exclude module: "logback-classic"
      exclude group: 'junit'
    }
    testImplementation "com.esotericsoftware:kryo"
    testImplementation "org.xerial:sqlite-jdbc"
    testImplementation "org.testcontainers:testcontainers"
    testImplementation "org.apache.httpcomponents.client5:httpclient5"
    testImplementation 'org.mock-server:mockserver-netty'
    testImplementation 'org.mock-server:mockserver-client-java'
    testImplementation 'javax.xml.bind:jaxb-api'
    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')
  }

  sourceSets {
    integration {
      java.srcDir "$projectDir/src/integration/java"
      resources.srcDir "$projectDir/src/integration/resources"
      compileClasspath += main.output + test.output
      runtimeClasspath += main.output + test.output
    }
  }

  configurations {
    integrationImplementation.extendsFrom testImplementation
    integrationRuntime.extendsFrom testRuntimeOnly
  }

  task integrationTest(type: Test) {
    testClassesDirs = sourceSets.integration.output.classesDirs
    classpath = sourceSets.integration.runtimeClasspath
    jvmArgs += project.property('extraJvmArgs')
  }
}

project(':iceberg-delta-lake') {
  // use integration test since we can take advantages of spark 3.3 to read datafiles of delta lake table
  // and create some tests involving sql query.
  configurations {
    integrationImplementation.extendsFrom testImplementation
    integrationRuntime.extendsFrom testRuntimeOnly
  }

  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')
    implementation project(':iceberg-parquet')
    implementation "com.fasterxml.jackson.core:jackson-databind"
    annotationProcessor "org.immutables:value"
    compileOnly "org.immutables:value"

    compileOnly "io.delta:delta-standalone_${scalaVersion}"

    compileOnly("org.apache.hadoop:hadoop-common") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'javax.servlet', module: 'servlet-api'
      exclude group: 'com.google.code.gson', module: 'gson'
    }

    // The newest version of delta-core uses Spark 3.3.*. Since its only for test, we do
    // not need to include older version of delta-core
    if (sparkVersions.contains("3.3")) {
      integrationImplementation "io.delta:delta-core_${scalaVersion}"
      integrationImplementation project(path: ":iceberg-spark:iceberg-spark-3.3_${scalaVersion}")
      integrationImplementation("org.apache.hadoop:hadoop-minicluster") {
        exclude group: 'org.apache.avro', module: 'avro'
        // to make sure netty libs only come from project(':iceberg-arrow')
        exclude group: 'io.netty', module: 'netty-buffer'
        exclude group: 'io.netty', module: 'netty-common'
      }
      integrationImplementation project(path: ':iceberg-hive-metastore')
      integrationImplementation project(path: ':iceberg-hive-metastore', configuration: 'testArtifacts')
      integrationImplementation("org.apache.spark:spark-hive_${scalaVersion}:3.3.2") {
        exclude group: 'org.apache.avro', module: 'avro'
        exclude group: 'org.apache.arrow'
        exclude group: 'org.apache.parquet'
        // to make sure netty libs only come from project(':iceberg-arrow')
        exclude group: 'io.netty', module: 'netty-buffer'
        exclude group: 'io.netty', module: 'netty-common'
        exclude group: 'org.roaringbitmap'
      }
    }
  }

  // The newest version of delta-core uses Spark 3.3.*. The integration test should only be built
  // if iceberg-spark-3.3 is available
  if (sparkVersions.contains("3.3")) {
    sourceSets {
      integration {
        java.srcDir "$projectDir/src/integration/java"
        resources.srcDir "$projectDir/src/integration/resources"
        compileClasspath += main.output + test.output
        runtimeClasspath += main.output + test.output
      }
    }

    task integrationTest(type: Test) {
      testClassesDirs = sourceSets.integration.output.classesDirs
      classpath = sourceSets.integration.runtimeClasspath
      jvmArgs += project.property('extraJvmArgs')
    }
    check.dependsOn integrationTest
  }
}

project(':iceberg-gcp') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')

    implementation platform('com.google.cloud:libraries-bom')
    implementation 'com.google.cloud:google-cloud-storage'

    testImplementation 'com.google.cloud:google-cloud-nio'

    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')

    testImplementation("org.apache.hadoop:hadoop-common") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'javax.servlet', module: 'servlet-api'
      exclude group: 'com.google.code.gson', module: 'gson'
    }
    testImplementation "com.esotericsoftware:kryo"
  }
}

project(':iceberg-hive-metastore') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    implementation project(':iceberg-core')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    annotationProcessor "org.immutables:value"
    compileOnly "org.immutables:value"

    implementation "com.github.ben-manes.caffeine:caffeine"

    compileOnly "org.apache.avro:avro"

    compileOnly("org.apache.hive:hive-metastore") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'org.pentaho' // missing dependency
      exclude group: 'org.apache.hbase'
      exclude group: 'org.apache.logging.log4j'
      exclude group: 'co.cask.tephra'
      exclude group: 'com.google.code.findbugs', module: 'jsr305'
      exclude group: 'org.eclipse.jetty.aggregate', module: 'jetty-all'
      exclude group: 'org.eclipse.jetty.orbit', module: 'javax.servlet'
      exclude group: 'org.apache.parquet', module: 'parquet-hadoop-bundle'
      exclude group: 'com.tdunning', module: 'json'
      exclude group: 'javax.transaction', module: 'transaction-api'
      exclude group: 'com.zaxxer', module: 'HikariCP'
    }

    // By default, hive-exec is a fat/uber jar and it exports a guava library
    // that's really old. We use the core classifier to be able to override our guava
    // version. Luckily, hive-exec seems to work okay so far with this version of guava
    // See: https://github.com/apache/hive/blob/master/ql/pom.xml#L911 for more context.
    testImplementation("org.apache.hive:hive-exec::core") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'org.pentaho' // missing dependency
      exclude group: 'org.apache.hive', module: 'hive-llap-tez'
      exclude group: 'org.apache.logging.log4j'
      exclude group: 'com.google.protobuf', module: 'protobuf-java'
      exclude group: 'org.apache.calcite'
      exclude group: 'org.apache.calcite.avatica'
      exclude group: 'com.google.code.findbugs', module: 'jsr305'
    }

    testImplementation("org.apache.hive:hive-metastore") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude group: 'org.pentaho' // missing dependency
      exclude group: 'org.apache.hbase'
      exclude group: 'org.apache.logging.log4j'
      exclude group: 'co.cask.tephra'
      exclude group: 'com.google.code.findbugs', module: 'jsr305'
      exclude group: 'org.eclipse.jetty.aggregate', module: 'jetty-all'
      exclude group: 'org.eclipse.jetty.orbit', module: 'javax.servlet'
      exclude group: 'org.apache.parquet', module: 'parquet-hadoop-bundle'
      exclude group: 'com.tdunning', module: 'json'
      exclude group: 'javax.transaction', module: 'transaction-api'
      exclude group: 'com.zaxxer', module: 'HikariCP'
    }

    compileOnly("org.apache.hadoop:hadoop-client") {
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
    }

    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
  }
}

project(':iceberg-orc') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')
    implementation("org.apache.avro:avro") {
      exclude group: 'org.tukaani' // xz compression is not supported
    }

    implementation("org.apache.orc:orc-core::nohive") {
      exclude group: 'org.apache.hadoop'
      exclude group: 'commons-lang'
      // These artifacts are shaded and included in the orc-core fat jar
      exclude group: 'com.google.protobuf', module: 'protobuf-java'
      exclude group: 'org.apache.hive', module: 'hive-storage-api'
    }

    compileOnly("org.apache.hadoop:hadoop-common") {
      exclude group: 'commons-beanutils'
      exclude group: 'org.apache.avro', module: 'avro'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
    }
    compileOnly("org.apache.hadoop:hadoop-client") {
      exclude group: 'org.apache.avro', module: 'avro'
    }

    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')
    testImplementation project(':iceberg-common')
    testImplementation 'org.apache.orc:orc-tools'
  }
}

project(':iceberg-parquet') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-core')
    implementation project(':iceberg-common')

    implementation("org.apache.parquet:parquet-avro") {
      exclude group: 'org.apache.avro', module: 'avro'
      // already shaded by Parquet
      exclude group: 'it.unimi.dsi'
      exclude group: 'org.codehaus.jackson'
    }

    compileOnly "org.apache.avro:avro"
    compileOnly("org.apache.hadoop:hadoop-client") {
      exclude group: 'org.apache.avro', module: 'avro'
    }

    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')
  }
}

project(':iceberg-arrow') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-core')
    implementation project(':iceberg-parquet')

    implementation("org.apache.arrow:arrow-vector") {
      exclude group: 'io.netty', module: 'netty-buffer'
      exclude group: 'io.netty', module: 'netty-common'
      exclude group: 'com.google.code.findbugs', module: 'jsr305'
    }
    implementation("org.apache.arrow:arrow-memory-netty") {
      exclude group: 'com.google.code.findbugs', module: 'jsr305'
      exclude group: 'io.netty', module: 'netty-common'
      exclude group: 'io.netty', module: 'netty-buffer'
    }

    runtimeOnly("io.netty:netty-buffer")

    implementation("org.apache.parquet:parquet-avro") {
      exclude group: 'org.apache.avro', module: 'avro'
      // already shaded by Parquet
      exclude group: 'it.unimi.dsi'
      exclude group: 'org.codehaus.jackson'
    }

    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')
    // To run ArrowReaderTest test cases, :netty-common is needed.
    // We import :netty-common through :arrow-memory-netty
    // so that the same version as used by the :arrow-memory-netty module is picked.
    testImplementation("org.apache.arrow:arrow-memory-netty")
    testImplementation("org.apache.hadoop:hadoop-common")
    testImplementation("org.apache.hadoop:hadoop-mapreduce-client-core")
  }
}

project(':iceberg-pig') {
  dependencies {
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')
    implementation project(':iceberg-parquet')

    implementation("org.apache.parquet:parquet-avro") {
      exclude group: 'org.apache.avro', module: 'avro'
      // already shaded by Parquet
      exclude group: 'it.unimi.dsi'
      exclude group: 'org.codehaus.jackson'
    }

    compileOnly("org.apache.pig:pig") {
      exclude group: "junit", module: "junit"
    }
    compileOnly("org.apache.hadoop:hadoop-mapreduce-client-core")
    compileOnly("org.apache.hadoop:hadoop-client") {
      exclude group: 'org.apache.avro', module: 'avro'
    }

    testImplementation("org.apache.hadoop:hadoop-minicluster") {
      exclude group: 'org.apache.avro', module: 'avro'
    }
  }
}

project(':iceberg-nessie') {
  test {
    useJUnitPlatform()
  }

  dependencies {
    api project(':iceberg-api')
    implementation project(':iceberg-common')
    implementation project(':iceberg-core')
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    implementation "org.projectnessie.nessie:nessie-client"
    implementation "com.fasterxml.jackson.core:jackson-databind"
    implementation "com.fasterxml.jackson.core:jackson-core"

    testImplementation "org.projectnessie.nessie:nessie-jaxrs-testextension"
    testImplementation "org.projectnessie.nessie:nessie-versioned-persist-in-memory"
    testImplementation "org.projectnessie.nessie:nessie-versioned-persist-in-memory-test"
    // Need to "pull in" el-api explicitly :(
    testImplementation "jakarta.el:jakarta.el-api"

    compileOnly "org.apache.hadoop:hadoop-common"
    testImplementation "org.apache.avro:avro"

    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')

    // Only there to prevent "warning: unknown enum constant SchemaType.OBJECT" compile messages
    compileOnly "org.eclipse.microprofile.openapi:microprofile-openapi-api:3.1"
    testCompileOnly "org.eclipse.microprofile.openapi:microprofile-openapi-api:3.0"
  }
}

project(':iceberg-dell') {
  dependencies {
    implementation project(':iceberg-core')
    implementation project(':iceberg-common')
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    compileOnly 'com.emc.ecs:object-client-bundle'

    testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')
    testImplementation "javax.xml.bind:jaxb-api"
    testImplementation "javax.activation:activation"
    testImplementation "org.glassfish.jaxb:jaxb-runtime"
  }
}

project(':iceberg-snowflake') {
  test {
    useJUnitPlatform()
  }

  dependencies {
    implementation project(':iceberg-core')
    implementation project(':iceberg-common')
    implementation project(path: ':iceberg-bundled-guava', configuration: 'shadow')
    implementation "com.fasterxml.jackson.core:jackson-databind"
    implementation "com.fasterxml.jackson.core:jackson-core"

    runtimeOnly("net.snowflake:snowflake-jdbc")

    testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')
  }
}

@Memoized
boolean versionFileExists() {
  return file('version.txt').exists()
}

@Memoized
String getVersionFromFile() {
  return file('version.txt').text.trim()
}

String getProjectVersion() {
  if (versionFileExists()) {
    return getVersionFromFile()
  }

  try {
    // we're fetching the version from the latest tag (prefixed with 'apache-iceberg-'),
    // which can look like this: '0.13.0-2-g805400f0.dirty' but we're only interested in the MAJOR.MINOR.PATCH part
    String version = gitVersion(prefix: 'apache-iceberg-')
    Pattern pattern = Pattern.compile("^([0-9]+)\\.([0-9]+)\\.([0-9]+)(.*)?\$")
    Matcher matcher = pattern.matcher(version)
    if (matcher.matches()) {
      // bump the MINOR version and always set the PATCH version to 0
      return matcher.group(1) + "." + (Integer.valueOf(matcher.group(2)) + 1) + ".0-SNAPSHOT"
    }
    return version
  } catch (Exception e) {
    throw new Exception("Neither version.txt nor git version exists: " + e.getMessage(), e)
  }
}

String getJavadocVersion() {
  if (versionFileExists()) {
    return getVersionFromFile()
  }

  try {
    // use the branch name in place of version in Javadoc
    return versionDetails().branchName
  } catch (NullPointerException e) {
    throw new Exception("Neither version.txt nor git version exists")
  }
}

apply from: 'jmh.gradle'
apply from: 'baseline.gradle'
apply from: 'deploy.gradle'
apply from: 'tasks.gradle'

