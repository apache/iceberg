diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java b/arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java
index 6e982e7f..6169d97c 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java
@@ -37,9 +37,6 @@ import org.apache.iceberg.types.Types.MapType;
 import org.apache.iceberg.types.Types.NestedField;
 import org.apache.iceberg.types.Types.StructType;
 
-import static org.apache.iceberg.types.Types.NestedField.optional;
-import static org.apache.iceberg.types.Types.NestedField.required;
-
 
 public class ArrowSchemaUtil {
   static final String ORIGINAL_TYPE = "originalType";
@@ -56,7 +53,7 @@ public class ArrowSchemaUtil {
    * @return arrow schema
    */
   public static Schema convert(final org.apache.iceberg.Schema schema) {
-    final ImmutableList.Builder<Field> fields = ImmutableList.builder();
+    ImmutableList.Builder<Field> fields = ImmutableList.builder();
 
     for (NestedField f : schema.columns()) {
       fields.add(convert(f));
@@ -81,10 +78,10 @@ public class ArrowSchemaUtil {
         arrowType = ArrowType.Bool.INSTANCE;
         break;
       case INTEGER:
-        arrowType = new ArrowType.Int(Integer.SIZE, true);
+        arrowType = new ArrowType.Int(Integer.SIZE, true /* signed */);
         break;
       case LONG:
-        arrowType = new ArrowType.Int(Long.SIZE, true);
+        arrowType = new ArrowType.Int(Long.SIZE, true /* signed */);
         break;
       case FLOAT:
         arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE);
@@ -103,7 +100,8 @@ public class ArrowSchemaUtil {
         arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);
         break;
       case TIMESTAMP:
-        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND, "UTC");
+        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND,
+            ((Types.TimestampType) field.type()).shouldAdjustToUTC() ? "UTC" : null);
         break;
       case DATE:
         arrowType = new ArrowType.Date(DateUnit.DAY);
@@ -129,17 +127,17 @@ public class ArrowSchemaUtil {
         metadata = ImmutableMap.of(ORIGINAL_TYPE, MAP_TYPE);
         final MapType mapType = field.type().asMapType();
         arrowType = ArrowType.List.INSTANCE;
-
         final List<Field> entryFields = Lists.newArrayList(
-            convert(required(0, MAP_KEY, mapType.keyType())),
-            convert(optional(0, MAP_VALUE, mapType.valueType()))
+            convert(NestedField.required(0, MAP_KEY, mapType.keyType())),
+            convert(NestedField.optional(0, MAP_VALUE, mapType.valueType()))
         );
 
         final Field entry = new Field("",
-            new FieldType(true, new ArrowType.Struct(), null), entryFields);
+            new FieldType(true, ArrowType.Struct.INSTANCE, null), entryFields);
         children.add(entry);
         break;
-      default: throw new UnsupportedOperationException("Unsupported field type: " + field);
+      default:
+        throw new UnsupportedOperationException("Unsupported field type: " + field);
     }
 
     return new Field(field.name(), new FieldType(field.isOptional(), arrowType, null, metadata), children);
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergArrowVectors.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergArrowVectors.java
new file mode 100644
index 00000000..92be8413
--- /dev/null
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergArrowVectors.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.arrow.vectorized;
+
+import org.apache.arrow.memory.BufferAllocator;
+import org.apache.arrow.vector.DecimalVector;
+import org.apache.arrow.vector.VarBinaryVector;
+import org.apache.arrow.vector.VarCharVector;
+
+public class IcebergArrowVectors {
+
+  /**
+   * Extension of Arrow's @{@link DecimalVector}. The whole reason of having this implementation is to override the
+   * expensive {@link DecimalVector#isSet(int)} method used by  {@link DecimalVector#getObject(int)}.
+   */
+  public static class DecimalArrowVector extends DecimalVector {
+    private NullabilityHolder nullabilityHolder;
+
+    public DecimalArrowVector(
+        String name,
+        BufferAllocator allocator, int precision, int scale) {
+      super(name, allocator, precision, scale);
+    }
+
+    /**
+     * Same as {@link #isNull(int)}.
+     *
+     * @param index position of element
+     * @return 1 if element at given index is not null, 0 otherwise
+     */
+    @Override
+    public int isSet(int index) {
+      return nullabilityHolder.isNullAt(index) ^ 1;
+    }
+
+    public void setNullabilityHolder(NullabilityHolder nullabilityHolder) {
+      this.nullabilityHolder = nullabilityHolder;
+    }
+  }
+
+  /**
+   * Extension of Arrow's @{@link VarBinaryVector}. The whole reason of having this implementation is to override the
+   * expensive {@link VarBinaryVector#isSet(int)} method.
+   */
+  public static class VarBinaryArrowVector extends VarBinaryVector {
+    private NullabilityHolder nullabilityHolder;
+
+    public VarBinaryArrowVector(
+        String name,
+        BufferAllocator allocator) {
+      super(name, allocator);
+    }
+
+    /**
+     * Same as {@link #isNull(int)}.
+     *
+     * @param index position of element
+     * @return 1 if element at given index is not null, 0 otherwise
+     */
+    public int isSet(int index) {
+      return nullabilityHolder.isNullAt(index) ^ 1;
+    }
+
+    public void setNullabilityHolder(NullabilityHolder nullabilityHolder) {
+      this.nullabilityHolder = nullabilityHolder;
+    }
+  }
+
+  /**
+   * Extension of Arrow's @{@link VarCharVector}. The reason of having this implementation is to override the expensive
+   * {@link VarCharVector#isSet(int)} method.
+   */
+  public static class VarcharArrowVector extends VarCharVector {
+
+    private NullabilityHolder nullabilityHolder;
+
+    public VarcharArrowVector(
+        String name,
+        BufferAllocator allocator) {
+      super(name, allocator);
+    }
+
+    /**
+     * Same as {@link #isNull(int)}.
+     *
+     * @param index position of element
+     * @return 1 if element at given index is not null, 0 otherwise
+     */
+    public int isSet(int index) {
+      return nullabilityHolder.isNullAt(index) ^ 1;
+    }
+
+    public void setNullabilityHolder(NullabilityHolder nullabilityHolder) {
+      this.nullabilityHolder = nullabilityHolder;
+    }
+  }
+
+}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergDecimalArrowVector.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergDecimalArrowVector.java
deleted file mode 100644
index 967fce5e..00000000
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergDecimalArrowVector.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.iceberg.arrow.vectorized;
-
-import org.apache.arrow.memory.BufferAllocator;
-import org.apache.arrow.vector.DecimalVector;
-
-/**
- * Extension of Arrow's @{@link DecimalVector}. The whole reason of having this implementation is to override the
- * expensive {@link DecimalVector#isSet(int)} method used by  {@link DecimalVector#getObject(int)}.
- */
-public class IcebergDecimalArrowVector extends DecimalVector {
-  private NullabilityHolder nullabilityHolder;
-
-  public IcebergDecimalArrowVector(
-      String name,
-      BufferAllocator allocator, int precision, int scale) {
-    super(name, allocator, precision, scale);
-  }
-
-  /**
-   * Same as {@link #isNull(int)}.
-   *
-   * @param index position of element
-   * @return 1 if element at given index is not null, 0 otherwise
-   */
-  @Override
-  public int isSet(int index) {
-    return nullabilityHolder.isNullAt(index) ? 0 : 1;
-  }
-
-  public void setNullabilityHolder(NullabilityHolder nullabilityHolder) {
-    this.nullabilityHolder = nullabilityHolder;
-  }
-}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergVarBinaryArrowVector.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergVarBinaryArrowVector.java
deleted file mode 100644
index b083ae92..00000000
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergVarBinaryArrowVector.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.iceberg.arrow.vectorized;
-
-import org.apache.arrow.memory.BufferAllocator;
-import org.apache.arrow.vector.VarBinaryVector;
-
-/**
- * Extension of Arrow's @{@link VarBinaryVector}. The whole reason of having this implementation is to override the
- * expensive {@link VarBinaryVector#isSet(int)} method.
- */
-public class IcebergVarBinaryArrowVector extends VarBinaryVector {
-  private NullabilityHolder nullabilityHolder;
-
-  public IcebergVarBinaryArrowVector(
-      String name,
-      BufferAllocator allocator) {
-    super(name, allocator);
-  }
-
-  /**
-   * Same as {@link #isNull(int)}.
-   *
-   * @param index position of element
-   * @return 1 if element at given index is not null, 0 otherwise
-   */
-  public int isSet(int index) {
-    return nullabilityHolder.isNullAt(index) ? 0 : 1;
-  }
-
-  public void setNullabilityHolder(NullabilityHolder nullabilityHolder) {
-    this.nullabilityHolder = nullabilityHolder;
-  }
-}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergVarcharArrowVector.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergVarcharArrowVector.java
deleted file mode 100644
index c5858d94..00000000
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergVarcharArrowVector.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.iceberg.arrow.vectorized;
-
-import org.apache.arrow.memory.BufferAllocator;
-import org.apache.arrow.vector.VarCharVector;
-
-/**
- * Extension of Arrow's @{@link VarCharVector}. The reason of having this implementation is to override the expensive
- * {@link VarCharVector#isSet(int)} method.
- */
-public class IcebergVarcharArrowVector extends VarCharVector {
-
-  private NullabilityHolder nullabilityHolder;
-
-  public IcebergVarcharArrowVector(
-      String name,
-      BufferAllocator allocator) {
-    super(name, allocator);
-  }
-
-  /**
-   * Same as {@link #isNull(int)}.
-   *
-   * @param index position of element
-   * @return 1 if element at given index is not null, 0 otherwise
-   */
-  public int isSet(int index) {
-    return nullabilityHolder.isNullAt(index) ? 0 : 1;
-  }
-
-  public void setNullabilityHolder(NullabilityHolder nullabilityHolder) {
-    this.nullabilityHolder = nullabilityHolder;
-  }
-}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java
index ec846e4b..b1da2e4a 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java
@@ -20,19 +20,24 @@
 package org.apache.iceberg.arrow.vectorized;
 
 public class NullabilityHolder {
-  private final boolean[] isNull;
+  private final byte[] isNull;
   private int numNulls;
 
-  public NullabilityHolder(int batchSize) {
-    this.isNull = new boolean[batchSize];
+  public NullabilityHolder(int size) {
+    this.isNull = new byte[size];
   }
 
   public void setNull(int idx) {
-    isNull[idx] = true;
+    isNull[idx] = 1;
     numNulls++;
   }
 
-  public boolean isNullAt(int idx) {
+  public void setNotNull(int idx) {
+    isNull[idx] = 0;
+    numNulls++;
+  }
+
+  public byte isNullAt(int idx) {
     return isNull[idx];
   }
 
@@ -43,4 +48,8 @@ public class NullabilityHolder {
   public int numNulls() {
     return numNulls;
   }
+
+  public void reset() {
+    numNulls = 0;
+  }
 }
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java
index 33e74275..ce485fee 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java
@@ -52,11 +52,11 @@ public class VectorHolder {
     this.nullabilityHolder = holder;
   }
 
-  public ColumnDescriptor getDescriptor() {
+  public ColumnDescriptor descriptor() {
     return columnDescriptor;
   }
 
-  public FieldVector getVector() {
+  public FieldVector vector() {
     return vector;
   }
 
@@ -64,11 +64,11 @@ public class VectorHolder {
     return isDictionaryEncoded;
   }
 
-  public Dictionary getDictionary() {
+  public Dictionary dictionary() {
     return dictionary;
   }
 
-  public NullabilityHolder getNullabilityHolder() {
+  public NullabilityHolder nullabilityHolder() {
     return nullabilityHolder;
   }
 }
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java
index 7f82183c..26705ff6 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java
@@ -130,33 +130,34 @@ public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {
   public VectorHolder read(int numValsToRead) {
     if (vec == null || !reuseContainers) {
       typeWidth = allocateFieldVector();
+      nullabilityHolder = new NullabilityHolder(batchSize);
     }
     vec.setValueCount(0);
-    nullabilityHolder = new NullabilityHolder(batchSize);
+    nullabilityHolder.reset();
     if (vectorizedColumnIterator.hasNext()) {
       if (allPagesDictEncoded) {
         vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);
       } else {
         if (isFixedLengthDecimal) {
           vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);
-          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);
+          ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);
         } else if (isFixedWidthBinary) {
           // Fixed width binary type values are stored in an IcebergVarBinaryArrowVector as well
-          if (vec instanceof IcebergVarBinaryArrowVector) {
-            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);
+          if (vec instanceof IcebergArrowVectors.VarBinaryArrowVector) {
+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);
           }
           vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);
         } else if (isVarWidthType) {
-          if (vec instanceof IcebergVarcharArrowVector) {
-            ((IcebergVarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);
-          } else if (vec instanceof IcebergVarBinaryArrowVector) {
-            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);
+          if (vec instanceof IcebergArrowVectors.VarcharArrowVector) {
+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);
+          } else if (vec instanceof IcebergArrowVectors.VarBinaryArrowVector) {
+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);
           }
           vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);
         } else if (isBooleanType) {
           vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);
         } else if (isPaddedDecimal) {
-          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);
+          ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);
           vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);
         } else if (isIntType) {
           vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);
@@ -193,7 +194,7 @@ public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {
           case JSON:
           case UTF8:
           case BSON:
-            this.vec = new IcebergVarcharArrowVector(icebergField.name(), rootAlloc);
+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);
             //TODO: Possibly use the uncompressed page size info to set the initial capacity
             vec.setInitialCapacity(batchSize * 10);
             vec.allocateNewSafe();
@@ -219,7 +220,7 @@ public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {
             return BigIntVector.TYPE_WIDTH;
           case DECIMAL:
             DecimalMetadata decimal = primitive.getDecimalMetadata();
-            this.vec = new IcebergDecimalArrowVector(icebergField.name(), rootAlloc, decimal.getPrecision(),
+            this.vec = new IcebergArrowVectors.DecimalArrowVector(icebergField.name(), rootAlloc, decimal.getPrecision(),
                 decimal.getScale());
             ((DecimalVector) vec).allocateNew(batchSize);
             switch (primitive.getPrimitiveTypeName()) {
@@ -242,13 +243,13 @@ public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {
         switch (primitive.getPrimitiveTypeName()) {
           case FIXED_LEN_BYTE_ARRAY:
             int len = ((Types.FixedType) icebergField.type()).length();
-            this.vec = new IcebergVarBinaryArrowVector(icebergField.name(), rootAlloc);
+            this.vec = new IcebergArrowVectors.VarBinaryArrowVector(icebergField.name(), rootAlloc);
             int factor = (len + DEFAULT_RECORD_BYTE_COUNT - 1) / DEFAULT_RECORD_BYTE_COUNT;
             vec.setInitialCapacity(batchSize * factor);
             vec.allocateNew();
             return len;
           case BINARY:
-            this.vec = new IcebergVarBinaryArrowVector(icebergField.name(), rootAlloc);
+            this.vec = new IcebergArrowVectors.VarBinaryArrowVector(icebergField.name(), rootAlloc);
             //TODO: Possibly use the uncompressed page size info to set the initial capacity
             vec.setInitialCapacity(batchSize * 10);
             vec.allocateNewSafe();
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java
new file mode 100644
index 00000000..c361d9d8
--- /dev/null
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java
@@ -0,0 +1,233 @@
+package org.apache.iceberg.arrow.vectorized.parquet;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import org.apache.parquet.Preconditions;
+import org.apache.parquet.bytes.ByteBufferInputStream;
+import org.apache.parquet.bytes.BytesUtils;
+import org.apache.parquet.column.values.ValuesReader;
+import org.apache.parquet.column.values.bitpacking.BytePacker;
+import org.apache.parquet.column.values.bitpacking.Packer;
+import org.apache.parquet.io.ParquetDecodingException;
+
+/**
+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a
+ * time. This is based off of the version in Apache Spark with these changes:
+ * <p>
+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>
+ * <tr>If all pages of a column within the row group are not dictionary encoded, then
+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>
+ * </p>
+ */
+public class BaseVectorizedParquetValuesReader extends ValuesReader {
+  // Current decoding mode. The encoded data contains groups of either run length encoded data
+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and
+  // the number of values in the group.
+  enum MODE {
+    RLE,
+    PACKED
+  }
+
+  // Encoded data.
+  private ByteBufferInputStream inputStream;
+
+  // bit/byte width of decoded data and utility to batch unpack them.
+  private int bitWidth;
+  private int bytesWidth;
+  private BytePacker packer;
+
+  // Current decoding mode and values
+  MODE mode;
+  int currentCount;
+  int currentValue;
+
+  // Buffer of decoded values if the values are PACKED.
+  int[] packedValuesBuffer = new int[16];
+  int packedValuesBufferIdx = 0;
+
+  // If true, the bit width is fixed. This decoder is used in different places and this also
+  // controls if we need to read the bitwidth from the beginning of the data stream.
+  private final boolean fixedWidth;
+  private final boolean readLength;
+  final int maxDefLevel;
+
+  public BaseVectorizedParquetValuesReader(int maxDefLevel) {
+    this.maxDefLevel = maxDefLevel;
+    this.fixedWidth = false;
+    this.readLength = false;
+  }
+
+  public BaseVectorizedParquetValuesReader(
+      int bitWidth,
+      int maxDefLevel) {
+    this.fixedWidth = true;
+    this.readLength = bitWidth != 0;
+    this.maxDefLevel = maxDefLevel;
+    init(bitWidth);
+  }
+
+  public BaseVectorizedParquetValuesReader(
+      int bw,
+      boolean rl,
+      int mdl) {
+    this.fixedWidth = true;
+    this.readLength = rl;
+    this.maxDefLevel = mdl;
+    init(bw);
+  }
+
+  @Override
+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {
+    this.inputStream = in;
+    if (fixedWidth) {
+      // initialize for repetition and definition levels
+      if (readLength) {
+        int length = readIntLittleEndian();
+        this.inputStream = in.sliceStream(length);
+      }
+    } else {
+      // initialize for values
+      if (in.available() > 0) {
+        init(in.read());
+      }
+    }
+    if (bitWidth == 0) {
+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.
+      this.mode = MODE.RLE;
+      this.currentCount = valueCount;
+      this.currentValue = 0;
+    } else {
+      this.currentCount = 0;
+    }
+  }
+
+  /**
+   * Initializes the internal state for decoding ints of `bitWidth`.
+   */
+  private void init(int bw) {
+    Preconditions.checkArgument(bw >= 0 && bw <= 32, "bitWidth must be >= 0 and <= 32");
+    this.bitWidth = bw;
+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);
+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);
+  }
+
+  /**
+   * Reads the next varint encoded int.
+   */
+  private int readUnsignedVarInt() throws IOException {
+    int value = 0;
+    int shift = 0;
+    int byteRead;
+    do {
+      byteRead = inputStream.read();
+      value |= (byteRead & 0x7F) << shift;
+      shift += 7;
+    } while ((byteRead & 0x80) != 0);
+    return value;
+  }
+
+  /**
+   * Reads the next 4 byte little endian int.
+   */
+  private int readIntLittleEndian() throws IOException {
+    int ch4 = inputStream.read();
+    int ch3 = inputStream.read();
+    int ch2 = inputStream.read();
+    int ch1 = inputStream.read();
+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);
+  }
+
+  /**
+   * Reads the next byteWidth little endian int.
+   */
+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {
+    switch (bytesWidth) {
+      case 0:
+        return 0;
+      case 1:
+        return inputStream.read();
+      case 2: {
+        int ch2 = inputStream.read();
+        int ch1 = inputStream.read();
+        return (ch1 << 8) + ch2;
+      }
+      case 3: {
+        int ch3 = inputStream.read();
+        int ch2 = inputStream.read();
+        int ch1 = inputStream.read();
+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);
+      }
+      case 4: {
+        return readIntLittleEndian();
+      }
+    }
+    throw new RuntimeException("Unreachable");
+  }
+
+  /**
+   * Reads the next group.
+   */
+  void readNextGroup() {
+    try {
+      int header = readUnsignedVarInt();
+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;
+      switch (mode) {
+        case RLE:
+          this.currentCount = header >>> 1;
+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();
+          return;
+        case PACKED:
+          int numGroups = header >>> 1;
+          this.currentCount = numGroups * 8;
+
+          if (this.packedValuesBuffer.length < this.currentCount) {
+            this.packedValuesBuffer = new int[this.currentCount];
+          }
+          packedValuesBufferIdx = 0;
+          int valueIndex = 0;
+          while (valueIndex < this.currentCount) {
+            // values are bit packed 8 at a time, so reading bitWidth will always work
+            ByteBuffer buffer = inputStream.slice(bitWidth);
+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);
+            valueIndex += 8;
+          }
+          return;
+        default:
+          throw new ParquetDecodingException("not a valid mode " + this.mode);
+      }
+    } catch (IOException e) {
+      throw new ParquetDecodingException("Failed to read from input stream", e);
+    }
+  }
+
+  @Override
+  public boolean readBoolean() {
+    return this.readInteger() != 0;
+  }
+
+  @Override
+  public void skip() {
+    this.readInteger();
+  }
+
+  @Override
+  public int readValueDictionaryId() {
+    return readInteger();
+  }
+
+  @Override
+  public int readInteger() {
+    if (this.currentCount == 0) {
+      this.readNextGroup();
+    }
+
+    this.currentCount--;
+    switch (mode) {
+      case RLE:
+        return this.currentValue;
+      case PACKED:
+        return this.packedValuesBuffer[packedValuesBufferIdx++];
+    }
+    throw new RuntimeException("Unreachable");
+  }
+}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
new file mode 100644
index 00000000..7a88542b
--- /dev/null
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
@@ -0,0 +1,348 @@
+package org.apache.iceberg.arrow.vectorized.parquet;
+
+import io.netty.buffer.ArrowBuf;
+import org.apache.arrow.vector.BaseVariableWidthVector;
+import org.apache.arrow.vector.BitVectorHelper;
+import org.apache.arrow.vector.DecimalVector;
+import org.apache.arrow.vector.FieldVector;
+import org.apache.arrow.vector.IntVector;
+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;
+import org.apache.parquet.column.Dictionary;
+
+public class VectorizedDictionaryEncodedParquetValuesReader extends BaseVectorizedParquetValuesReader {
+
+  public VectorizedDictionaryEncodedParquetValuesReader(int maxDefLevel) {
+    super(maxDefLevel);
+  }
+
+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't
+  // check definition level.
+  void readBatchOfDictionaryIds(
+      final IntVector intVector,
+      final int numValsInVector,
+      final int numValuesToRead,
+      NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = numValsInVector;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int numValues = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < numValues; i++) {
+            intVector.set(idx, currentValue);
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < numValues; i++) {
+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);
+            nullabilityHolder.setNotNull(idx);
+            packedValuesBufferIdx++;
+            idx++;
+          }
+          break;
+      }
+      left -= numValues;
+      currentCount -= numValues;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedLongs(
+      FieldVector vector,
+      int index,
+      int numValuesToRead,
+      Dictionary dict,
+      NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int numValues = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < numValues; i++) {
+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < numValues; i++) {
+            vector.getDataBuffer()
+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= numValues;
+      currentCount -= numValues;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedIntegers(
+      FieldVector vector,
+      int typeWidth,
+      int index,
+      int numValuesToRead,
+      Dictionary dict,
+      NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      ArrowBuf dataBuffer = vector.getDataBuffer();
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedFloats(
+      FieldVector vector,
+      int typeWidth,
+      int index,
+      int numValuesToRead,
+      Dictionary dict,
+      NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedDoubles(
+      FieldVector vector,
+      int typeWidth,
+      int index,
+      int numValuesToRead,
+      Dictionary dict, NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedFixedWidthBinary(
+      FieldVector vector,
+      int typeWidth,
+      int index,
+      int numValuesToRead,
+      Dictionary dict,
+      NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());
+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            vector.getDataBuffer()
+                .setBytes(
+                    idx * typeWidth,
+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());
+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedFixedLengthDecimals(
+      FieldVector vector,
+      int typeWidth,
+      int index,
+      int numValuesToRead,
+      Dictionary dict, NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();
+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];
+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);
+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();
+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];
+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);
+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedVarWidthBinary(
+      FieldVector vector,
+      int index,
+      int numValuesToRead,
+      Dictionary dict, NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            ((BaseVariableWidthVector) vector).setSafe(idx, dict.decodeToBinary(currentValue).getBytesUnsafe());
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            ((BaseVariableWidthVector) vector).setSafe(
+                idx,
+                dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+
+  void readBatchOfDictionaryEncodedIntLongBackedDecimals(
+      FieldVector vector,
+      final int typeWidth,
+      int index,
+      int numValuesToRead,
+      Dictionary dict, NullabilityHolder nullabilityHolder) {
+    int left = numValuesToRead;
+    int idx = index;
+    while (left > 0) {
+      if (this.currentCount == 0) {
+        this.readNextGroup();
+      }
+      int num = Math.min(left, this.currentCount);
+      switch (mode) {
+        case RLE:
+          for (int i = 0; i < num; i++) {
+            ((DecimalVector) vector).set(
+                idx,
+                typeWidth == Integer.BYTES ? dict.decodeToInt(currentValue) : dict.decodeToLong(currentValue));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+        case PACKED:
+          for (int i = 0; i < num; i++) {
+            ((DecimalVector) vector).set(
+                idx,
+                typeWidth == Integer.BYTES ?
+                    dict.decodeToInt(currentValue)
+                    : dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));
+            nullabilityHolder.setNotNull(idx);
+            idx++;
+          }
+          break;
+      }
+      left -= num;
+      currentCount -= num;
+    }
+  }
+}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java
index f5c26249..5466ac0b 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java
@@ -72,7 +72,7 @@ public class VectorizedPageIterator {
   private VectorizedParquetValuesReader definitionLevelReader;
   private boolean eagerDecodeDictionary;
   private ValuesAsBytesReader plainValuesReader = null;
-  private VectorizedParquetValuesReader dictionaryEncodedValuesReader = null;
+  private VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader = null;
   private boolean allPagesDictEncoded;
 
   public void setPage(DataPage dataPage) {
@@ -91,7 +91,7 @@ public class VectorizedPageIterator {
       }
     });
     this.triplesRead = 0;
-    advance();
+    this.hasNext = triplesRead < triplesCount;
   }
 
   // Dictionary is set per row group
@@ -439,14 +439,6 @@ public class VectorizedPageIterator {
     return actualBatchSize;
   }
 
-  private void advance() {
-    if (triplesRead < triplesCount) {
-      this.hasNext = true;
-    } else {
-      this.hasNext = false;
-    }
-  }
-
   private void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount) {
     ValuesReader previousReader = plainValuesReader;
     this.eagerDecodeDictionary = dataEncoding.usesDictionary() && dictionary != null && !allPagesDictEncoded;
@@ -457,7 +449,7 @@ public class VectorizedPageIterator {
       }
       try {
         dictionaryEncodedValuesReader =
-            new VectorizedParquetValuesReader(desc.getMaxDefinitionLevel());
+            new VectorizedDictionaryEncodedParquetValuesReader(desc.getMaxDefinitionLevel());
         dictionaryEncodedValuesReader.initFromPage(valueCount, in);
       } catch (IOException e) {
         throw new ParquetDecodingException("could not read page in col " + desc, e);
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java
index b4b6dc20..57389ef5 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java
@@ -20,7 +20,6 @@
 package org.apache.iceberg.arrow.vectorized.parquet;
 
 import io.netty.buffer.ArrowBuf;
-import java.io.IOException;
 import java.nio.ByteBuffer;
 import org.apache.arrow.vector.BaseVariableWidthVector;
 import org.apache.arrow.vector.BitVector;
@@ -31,235 +30,16 @@ import org.apache.arrow.vector.IntVector;
 import org.apache.arrow.vector.VarBinaryVector;
 import org.apache.iceberg.arrow.vectorized.NullabilityHolder;
 import org.apache.iceberg.parquet.ValuesAsBytesReader;
-import org.apache.parquet.Preconditions;
-import org.apache.parquet.bytes.ByteBufferInputStream;
-import org.apache.parquet.bytes.BytesUtils;
 import org.apache.parquet.column.Dictionary;
-import org.apache.parquet.column.values.ValuesReader;
-import org.apache.parquet.column.values.bitpacking.BytePacker;
-import org.apache.parquet.column.values.bitpacking.Packer;
-import org.apache.parquet.io.ParquetDecodingException;
 
-/**
- * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a
- * time. This is based off of the version in Apache Spark with these changes:
- * <p>
- * <tr>Writes batches of values retrieved to Arrow vectors</tr>
- * <tr>If all pages of a column within the row group are not dictionary encoded, then
- * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>
- * </p>
- */
-public final class VectorizedParquetValuesReader extends ValuesReader {
-
-  // Current decoding mode. The encoded data contains groups of either run length encoded data
-  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and
-  // the number of values in the group.
-  private enum MODE {
-    RLE,
-    PACKED
-  }
-
-  // Encoded data.
-  private ByteBufferInputStream inputStream;
-
-  // bit/byte width of decoded data and utility to batch unpack them.
-  private int bitWidth;
-  private int bytesWidth;
-  private BytePacker packer;
-
-  // Current decoding mode and values
-  private MODE mode;
-  private int currentCount;
-  private int currentValue;
-
-  // Buffer of decoded values if the values are PACKED.
-  private int[] packedValuesBuffer = new int[16];
-  private int packedValuesBufferIdx = 0;
-
-  // If true, the bit width is fixed. This decoder is used in different places and this also
-  // controls if we need to read the bitwidth from the beginning of the data stream.
-  private final boolean fixedWidth;
-  private final boolean readLength;
-  private final int maxDefLevel;
-
-  public VectorizedParquetValuesReader(int maxDefLevel) {
-    this.maxDefLevel = maxDefLevel;
-    this.fixedWidth = false;
-    this.readLength = false;
-  }
+public final class VectorizedParquetValuesReader extends BaseVectorizedParquetValuesReader {
 
-  public VectorizedParquetValuesReader(
-      int bitWidth,
-      int maxDefLevel) {
-    this.fixedWidth = true;
-    this.readLength = bitWidth != 0;
-    this.maxDefLevel = maxDefLevel;
-    init(bitWidth);
+  public VectorizedParquetValuesReader(int bitWidth, int maxDefLevel) {
+    super(bitWidth, maxDefLevel);
   }
 
-  public VectorizedParquetValuesReader(
-      int bw,
-      boolean rl,
-      int mdl) {
-    this.fixedWidth = true;
-    this.readLength = rl;
-    this.maxDefLevel = mdl;
-    init(bw);
-  }
-
-  @Override
-  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {
-    this.inputStream = in;
-    if (fixedWidth) {
-      // initialize for repetition and definition levels
-      if (readLength) {
-        int length = readIntLittleEndian();
-        this.inputStream = in.sliceStream(length);
-      }
-    } else {
-      // initialize for values
-      if (in.available() > 0) {
-        init(in.read());
-      }
-    }
-    if (bitWidth == 0) {
-      // 0 bit width, treat this as an RLE run of valueCount number of 0's.
-      this.mode = MODE.RLE;
-      this.currentCount = valueCount;
-      this.currentValue = 0;
-    } else {
-      this.currentCount = 0;
-    }
-  }
-
-  /**
-   * Initializes the internal state for decoding ints of `bitWidth`.
-   */
-  private void init(int bw) {
-    Preconditions.checkArgument(bw >= 0 && bw <= 32, "bitWidth must be >= 0 and <= 32");
-    this.bitWidth = bw;
-    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);
-    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);
-  }
-
-  /**
-   * Reads the next varint encoded int.
-   */
-  private int readUnsignedVarInt() throws IOException {
-    int value = 0;
-    int shift = 0;
-    int byteRead;
-    do {
-      byteRead = inputStream.read();
-      value |= (byteRead & 0x7F) << shift;
-      shift += 7;
-    } while ((byteRead & 0x80) != 0);
-    return value;
-  }
-
-  /**
-   * Reads the next 4 byte little endian int.
-   */
-  private int readIntLittleEndian() throws IOException {
-    int ch4 = inputStream.read();
-    int ch3 = inputStream.read();
-    int ch2 = inputStream.read();
-    int ch1 = inputStream.read();
-    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);
-  }
-
-  /**
-   * Reads the next byteWidth little endian int.
-   */
-  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {
-    switch (bytesWidth) {
-      case 0:
-        return 0;
-      case 1:
-        return inputStream.read();
-      case 2: {
-        int ch2 = inputStream.read();
-        int ch1 = inputStream.read();
-        return (ch1 << 8) + ch2;
-      }
-      case 3: {
-        int ch3 = inputStream.read();
-        int ch2 = inputStream.read();
-        int ch1 = inputStream.read();
-        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);
-      }
-      case 4: {
-        return readIntLittleEndian();
-      }
-    }
-    throw new RuntimeException("Unreachable");
-  }
-
-  /**
-   * Reads the next group.
-   */
-  private void readNextGroup() {
-    try {
-      int header = readUnsignedVarInt();
-      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;
-      switch (mode) {
-        case RLE:
-          this.currentCount = header >>> 1;
-          this.currentValue = readIntLittleEndianPaddedOnBitWidth();
-          return;
-        case PACKED:
-          int numGroups = header >>> 1;
-          this.currentCount = numGroups * 8;
-
-          if (this.packedValuesBuffer.length < this.currentCount) {
-            this.packedValuesBuffer = new int[this.currentCount];
-          }
-          packedValuesBufferIdx = 0;
-          int valueIndex = 0;
-          while (valueIndex < this.currentCount) {
-            // values are bit packed 8 at a time, so reading bitWidth will always work
-            ByteBuffer buffer = inputStream.slice(bitWidth);
-            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);
-            valueIndex += 8;
-          }
-          return;
-        default:
-          throw new ParquetDecodingException("not a valid mode " + this.mode);
-      }
-    } catch (IOException e) {
-      throw new ParquetDecodingException("Failed to read from input stream", e);
-    }
-  }
-
-  @Override
-  public boolean readBoolean() {
-    return this.readInteger() != 0;
-  }
-
-  @Override
-  public void skip() {
-    this.readInteger();
-  }
-
-  @Override
-  public int readValueDictionaryId() {
-    return readInteger();
-  }
-
-  @Override
-  public int readInteger() {
-    if (this.currentCount == 0) {
-      this.readNextGroup();
-    }
-
-    this.currentCount--;
-    switch (mode) {
-      case RLE:
-        return this.currentValue;
-      case PACKED:
-        return this.packedValuesBuffer[packedValuesBufferIdx++];
-    }
-    throw new RuntimeException("Unreachable");
+  public VectorizedParquetValuesReader(int bitWidth, boolean repetitionLevel, int maxDefinitionLevel) {
+    super(bitWidth, repetitionLevel, maxDefinitionLevel);
   }
 
   public void readBatchOfDictionaryIds(
@@ -267,7 +47,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int numValsInVector,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader) {
     int idx = numValsInVector;
     int left = batchSize;
     while (left > 0) {
@@ -278,7 +58,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryIds(vector, idx, numValues, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());
           }
@@ -288,6 +68,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
           for (int i = 0; i < numValues; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
               vector.set(idx, dictionaryEncodedValuesReader.readInteger());
+              nullabilityHolder.setNotNull(idx);
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -300,39 +81,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't
-  // check definition level.
-  private void readDictionaryIdsInternal(
-      final IntVector intVector,
-      final int numValsInVector,
-      final int numValuesToRead) {
-    int left = numValuesToRead;
-    int idx = numValsInVector;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int numValues = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < numValues; i++) {
-            intVector.set(idx, currentValue);
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < numValues; i++) {
-            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);
-            packedValuesBufferIdx++;
-            idx++;
-          }
-          break;
-      }
-      left -= numValues;
-      currentCount -= numValues;
-    }
-  }
-
   public void readBatchOfLongs(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {
@@ -362,7 +110,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
                   valuesReader,
                   bufferIdx,
                   vector.getValidityBuffer(),
-                  vector.getDataBuffer());
+                  vector.getDataBuffer(), nullabilityHolder);
             } else {
               setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());
             }
@@ -381,7 +129,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -394,7 +142,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedLongs(vector, idx, numValues, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, numValues, validityBuffer);
           }
@@ -403,7 +151,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < numValues; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));
+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(dictionaryEncodedValuesReader.readInteger()));
             } else {
               setNull(nullabilityHolder, idx, validityBuffer);
             }
@@ -416,39 +164,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedLongsInternal(
-      FieldVector vector,
-      int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int numValues = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < numValues; i++) {
-            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < numValues; i++) {
-            vector.getDataBuffer()
-                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));
-            idx++;
-          }
-          break;
-      }
-      left -= numValues;
-      currentCount -= numValues;
-    }
-  }
-
   public void readBatchOfIntegers(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {
@@ -473,7 +188,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; ++i) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());
+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer(), nullabilityHolder);
             } else {
               setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());
             }
@@ -492,7 +207,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -504,7 +219,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedIntegers(vector, typeWidth, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());
           }
@@ -513,7 +228,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));
+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(dictionaryEncodedValuesReader.readInteger()));
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -526,39 +241,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedIntegersInternal(
-      FieldVector vector,
-      int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      ArrowBuf dataBuffer = vector.getDataBuffer();
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   public void readBatchOfFloats(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {
@@ -583,7 +265,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; ++i) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());
+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer(), nullabilityHolder);
             } else {
               setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());
             }
@@ -596,23 +278,13 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void setValue(
-      int typeWidth,
-      ValuesAsBytesReader valuesReader,
-      int bufferIdx,
-      ArrowBuf validityBuffer,
-      ArrowBuf dataBuffer) {
-    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));
-    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);
-  }
-
   public void readBatchOfDictionaryEncodedFloats(
       final FieldVector vector,
       final int numValsInVector,
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -625,7 +297,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedFloats(vector, typeWidth, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, validityBuffer);
           }
@@ -634,7 +306,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));
+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(dictionaryEncodedValuesReader.readInteger()));
             } else {
               setNull(nullabilityHolder, idx, validityBuffer);
             }
@@ -647,38 +319,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedFloatsInternal(
-      FieldVector vector,
-      int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   public void readBatchOfDoubles(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,
@@ -704,7 +344,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; ++i) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());
+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer(), nullabilityHolder);
             } else {
               setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());
             }
@@ -723,7 +363,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -735,7 +375,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedDoubles(vector, typeWidth, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());
           }
@@ -744,7 +384,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));
+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(dictionaryEncodedValuesReader.readInteger()));
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -757,38 +397,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedDoublesInternal(
-      FieldVector vector,
-      int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   public void readBatchOfFixedWidthBinary(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,
@@ -804,7 +412,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case RLE:
           if (currentValue == maxDefLevel) {
             for (int i = 0; i < num; i++) {
-              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);
+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx, nullabilityHolder);
               bufferIdx++;
             }
           } else {
@@ -815,7 +423,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);
+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx, nullabilityHolder);
             } else {
               setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());
             }
@@ -834,7 +442,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -846,7 +454,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedFixedWidthBinary(vector, typeWidth, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());
           }
@@ -856,7 +464,8 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
           for (int i = 0; i < num; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
               vector.getDataBuffer()
-                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());
+                  .setBytes(idx * typeWidth, dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).getBytesUnsafe());
+              nullabilityHolder.setNotNull(idx);
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -869,43 +478,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(
-      FieldVector vector,
-      int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());
-            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            vector.getDataBuffer()
-                .setBytes(
-                    idx * typeWidth,
-                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());
-            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   public void readBatchOfFixedLengthDecimals(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,
@@ -955,7 +527,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -967,7 +539,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedFixedLengthDecimals(vector, typeWidth, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());
           }
@@ -976,10 +548,11 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
         case PACKED:
           for (int i = 0; i < num; i++) {
             if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {
-              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();
+              byte[] decimalBytes = dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).getBytesUnsafe();
               byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];
               System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);
               ((DecimalVector) vector).setBigEndian(idx, vectorBytes);
+              nullabilityHolder.setNotNull(idx);
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -992,44 +565,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(
-      FieldVector vector,
-      int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();
-            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];
-            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);
-            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();
-            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];
-            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);
-            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   /**
    * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP) This
    * method reads batches of bytes from Parquet and writes them into the data buffer underneath the Arrow vector. It
@@ -1095,7 +630,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int numValsInVector,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader dictionaryEncodedValuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -1107,7 +642,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedVarWidthBinaryInternal(vector, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedVarWidthBinary(vector, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());
           }
@@ -1119,6 +654,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
               ((BaseVariableWidthVector) vector).setSafe(
                   idx,
                   dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).getBytesUnsafe());
+              nullabilityHolder.setNotNull(idx);
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -1131,39 +667,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedVarWidthBinaryInternal(
-      FieldVector vector,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            ((BaseVariableWidthVector) vector).setSafe(idx, dict.decodeToBinary(currentValue).getBytesUnsafe());
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            ((BaseVariableWidthVector) vector).setSafe(
-                idx,
-                dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   public void readBatchOfIntLongBackedDecimals(
       final FieldVector vector, final int numValsInVector,
       final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,
@@ -1215,7 +718,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       final int typeWidth,
       final int batchSize,
       NullabilityHolder nullabilityHolder,
-      VectorizedParquetValuesReader valuesReader,
+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,
       Dictionary dict) {
     int idx = numValsInVector;
     int left = batchSize;
@@ -1227,7 +730,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       switch (mode) {
         case RLE:
           if (currentValue == maxDefLevel) {
-            valuesReader.readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(vector, typeWidth, idx, num, dict);
+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedIntLongBackedDecimals(vector, typeWidth, idx, num, dict, nullabilityHolder);
           } else {
             setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());
           }
@@ -1239,8 +742,9 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
               ((DecimalVector) vector).set(
                   idx,
                   typeWidth == Integer.BYTES ?
-                      dict.decodeToInt(valuesReader.readInteger())
-                      : dict.decodeToLong(valuesReader.readInteger()));
+                      dict.decodeToInt(dictionaryEncodedValuesReader.readInteger())
+                      : dict.decodeToLong(dictionaryEncodedValuesReader.readInteger()));
+              nullabilityHolder.setNotNull(idx);
             } else {
               setNull(nullabilityHolder, idx, vector.getValidityBuffer());
             }
@@ -1253,44 +757,6 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(
-      FieldVector vector,
-      final int typeWidth,
-      int index,
-      int numValuesToRead,
-      Dictionary dict) {
-    int left = numValuesToRead;
-    int idx = index;
-    while (left > 0) {
-      if (this.currentCount == 0) {
-        this.readNextGroup();
-      }
-      int num = Math.min(left, this.currentCount);
-      switch (mode) {
-        case RLE:
-          for (int i = 0; i < num; i++) {
-            ((DecimalVector) vector).set(
-                idx,
-                typeWidth == Integer.BYTES ? dict.decodeToInt(currentValue) : dict.decodeToLong(currentValue));
-            idx++;
-          }
-          break;
-        case PACKED:
-          for (int i = 0; i < num; i++) {
-            ((DecimalVector) vector).set(
-                idx,
-                typeWidth == Integer.BYTES ?
-                    dict.decodeToInt(currentValue)
-                    : dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));
-            idx++;
-          }
-          break;
-      }
-      left -= num;
-      currentCount -= num;
-    }
-  }
-
   public void readBatchOfBooleans(
       final FieldVector vector,
       final int numValsInVector,
@@ -1332,14 +798,15 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void setBinaryInVector(
+  private static void setBinaryInVector(
       VarBinaryVector vector,
       int typeWidth,
       ValuesAsBytesReader valuesReader,
-      int bufferIdx) {
+      int bufferIdx, NullabilityHolder nullabilityHolder) {
     byte[] byteArray = new byte[typeWidth];
     valuesReader.getBuffer(typeWidth).get(byteArray);
     vector.setSafe(bufferIdx, byteArray);
+    nullabilityHolder.setNotNull(bufferIdx);
   }
 
   private void setNextNValuesInVector(
@@ -1350,6 +817,7 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     if (currentValue == maxDefLevel) {
       for (int i = 0; i < numValues; i++) {
         BitVectorHelper.setValidityBitToOne(validityBuffer, validityBufferIdx);
+        nullabilityHolder.setNotNull(validityBufferIdx);
         validityBufferIdx++;
       }
       ByteBuffer buffer = valuesReader.getBuffer(numValues * typeWidth);
@@ -1359,12 +827,12 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
     }
   }
 
-  private void setNull(NullabilityHolder nullabilityHolder, int bufferIdx, ArrowBuf validityBuffer) {
+  private static void setNull(NullabilityHolder nullabilityHolder, int bufferIdx, ArrowBuf validityBuffer) {
     nullabilityHolder.setNull(bufferIdx);
     BitVectorHelper.setValidityBit(validityBuffer, bufferIdx, 0);
   }
 
-  private void setNulls(NullabilityHolder nullabilityHolder, int idx, int numValues, ArrowBuf validityBuffer) {
+  private static void setNulls(NullabilityHolder nullabilityHolder, int idx, int numValues, ArrowBuf validityBuffer) {
     int bufferIdx = idx;
     for (int i = 0; i < numValues; i++) {
       nullabilityHolder.setNull(bufferIdx);
@@ -1372,4 +840,16 @@ public final class VectorizedParquetValuesReader extends ValuesReader {
       bufferIdx++;
     }
   }
+
+  private static void setValue(
+      int typeWidth,
+      ValuesAsBytesReader valuesReader,
+      int bufferIdx,
+      ArrowBuf validityBuffer,
+      ArrowBuf dataBuffer,
+      NullabilityHolder nullabilityHolder) {
+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));
+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);
+    nullabilityHolder.setNotNull(bufferIdx);
+  }
 }
diff --git a/arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java b/arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java
index f42ebde6..885304e4 100644
--- a/arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java
+++ b/arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java
@@ -27,37 +27,37 @@ import org.apache.iceberg.types.Types;
 import org.apache.iceberg.types.Types.BooleanType;
 import org.apache.iceberg.types.Types.DateType;
 import org.apache.iceberg.types.Types.DoubleType;
+import org.apache.iceberg.types.Types.FloatType;
+import org.apache.iceberg.types.Types.IntegerType;
 import org.apache.iceberg.types.Types.ListType;
 import org.apache.iceberg.types.Types.LongType;
 import org.apache.iceberg.types.Types.MapType;
 import org.apache.iceberg.types.Types.StringType;
 import org.apache.iceberg.types.Types.TimestampType;
+import org.apache.iceberg.types.Types.TimeType;
+import org.junit.Assert;
 import org.junit.Test;
 
-import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Bool;
-import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Date;
-import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.FloatingPoint;
-import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Int;
-import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.List;
-import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Timestamp;
-import static org.apache.iceberg.types.Types.NestedField.optional;
-import static org.apache.iceberg.types.Types.NestedField.required;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-
 
 public class ArrowSchemaUtilTest {
 
   @Test
   public void convertPrimitive() {
     Schema iceberg = new Schema(
-        optional(0, "i", Types.IntegerType.get()),
-        optional(1, "b", BooleanType.get()),
-        required(2, "d", DoubleType.get()),
-        required(3, "s", StringType.get()),
-        optional(4, "d2", DateType.get()),
-        optional(5, "ts", TimestampType.withoutZone())
-    );
+        Types.NestedField.optional(0, "i", IntegerType.get()),
+        Types.NestedField.optional(1, "b", BooleanType.get()),
+        Types.NestedField.required(2, "d", DoubleType.get()),
+        Types.NestedField.required(3, "s", StringType.get()),
+        Types.NestedField.optional(4, "d2", DateType.get()),
+        Types.NestedField.optional(5, "ts", TimestampType.withZone()),
+        Types.NestedField.optional(6, "l", LongType.get()),
+        Types.NestedField.optional(7, "f", FloatType.get()),
+        Types.NestedField.optional(8, "tt", TimeType.get()),
+        Types.NestedField.optional(9, "bt", Types.BinaryType.get()),
+        Types.NestedField.optional(10, "dt", Types.DecimalType.of(1, 1)),
+        Types.NestedField.optional(11, "st", Types.StructType.of()),
+        Types.NestedField.optional(12, "lt", Types.ListType.ofOptional(13, Types.IntegerType.get())),
+        Types.NestedField.optional(14, "mt", Types.MapType.ofOptional(15, 16, StringType.get(), IntegerType.get())));
 
     org.apache.arrow.vector.types.pojo.Schema arrow = ArrowSchemaUtil.convert(iceberg);
 
@@ -70,11 +70,11 @@ public class ArrowSchemaUtilTest {
   @Test
   public void convertComplex() {
     Schema iceberg = new Schema(
-        optional(0, "m", MapType.ofOptional(
+        Types.NestedField.optional(0, "m", MapType.ofOptional(
             1, 2, StringType.get(),
             LongType.get())
         ),
-        required(3, "m2", MapType.ofOptional(
+        Types.NestedField.required(3, "m2", MapType.ofOptional(
             4, 5, StringType.get(),
             ListType.ofOptional(6, TimestampType.withoutZone()))
         )
@@ -85,15 +85,15 @@ public class ArrowSchemaUtilTest {
     System.out.println(iceberg);
     System.out.println(arrow);
 
-    assertEquals(iceberg.columns().size(), arrow.getFields().size());
+    Assert.assertEquals(iceberg.columns().size(), arrow.getFields().size());
   }
 
   private void validate(Schema iceberg, org.apache.arrow.vector.types.pojo.Schema arrow) {
-    assertEquals(iceberg.columns().size(), arrow.getFields().size());
+    Assert.assertEquals(iceberg.columns().size(), arrow.getFields().size());
 
     for (Types.NestedField nf : iceberg.columns()) {
       Field field = arrow.findField(nf.name());
-      assertNotNull("Missing filed: " + nf, field);
+      Assert.assertNotNull("Missing filed: " + nf, field);
 
       validate(nf.type(), field.getType());
     }
@@ -101,23 +101,47 @@ public class ArrowSchemaUtilTest {
 
   private void validate(Type iceberg, ArrowType arrow) {
     switch (iceberg.typeId()) {
-      case BOOLEAN: assertEquals(Bool, arrow.getTypeID());
+      case BOOLEAN:
+        Assert.assertEquals(ArrowType.ArrowTypeID.Bool, arrow.getTypeID());
+        break;
+      case INTEGER:
+      case LONG:
+        Assert.assertEquals(ArrowType.ArrowTypeID.Int, arrow.getTypeID());
+        break;
+      case FLOAT:
+      case DOUBLE:
+        Assert.assertEquals(ArrowType.ArrowTypeID.FloatingPoint, arrow.getTypeID());
+        break;
+      case DATE:
+        Assert.assertEquals(ArrowType.ArrowTypeID.Date, arrow.getTypeID());
+        break;
+      case TIME:
+        Assert.assertEquals(ArrowType.ArrowTypeID.Time, arrow.getTypeID());
         break;
-      case INTEGER: assertEquals(Int, arrow.getTypeID());
+      case TIMESTAMP:
+        Assert.assertEquals(ArrowType.ArrowTypeID.Timestamp, arrow.getTypeID());
         break;
-      case LONG: assertEquals(Int, arrow.getTypeID());
+      case STRING:
+        Assert.assertEquals(ArrowType.ArrowTypeID.Utf8, arrow.getTypeID());
         break;
-      case DOUBLE: assertEquals(FloatingPoint, arrow.getTypeID());
+      case FIXED:
+      case BINARY:
+        Assert.assertEquals(ArrowType.Binary.TYPE_TYPE, arrow.getTypeID());
         break;
-      case STRING: assertEquals(ArrowType.Utf8.INSTANCE.getTypeID(), arrow.getTypeID());
+      case DECIMAL:
+        Assert.assertEquals(ArrowType.Decimal.TYPE_TYPE, arrow.getTypeID());
         break;
-      case DATE: assertEquals(Date, arrow.getTypeID());
+      case STRUCT:
+        Assert.assertEquals(ArrowType.Struct.TYPE_TYPE, arrow.getTypeID());
         break;
-      case TIMESTAMP: assertEquals(Timestamp, arrow.getTypeID());
+      case LIST:
+        Assert.assertEquals(ArrowType.List.TYPE_TYPE, arrow.getTypeID());
         break;
-      case MAP: assertEquals(List, arrow.getTypeID());
+      case MAP:
+        Assert.assertEquals(ArrowType.ArrowTypeID.List, arrow.getTypeID());
         break;
-      default: throw new UnsupportedOperationException("Check not implemented for type: " + iceberg);
+      default:
+        throw new UnsupportedOperationException("Check not implemented for type: " + iceberg);
     }
   }
 }
diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/PageIterator.java b/parquet/src/main/java/org/apache/iceberg/parquet/PageIterator.java
deleted file mode 100644
index bedb6738..00000000
--- a/parquet/src/main/java/org/apache/iceberg/parquet/PageIterator.java
+++ /dev/null
@@ -1,401 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.iceberg.parquet;
-
-import com.google.common.base.Preconditions;
-import java.io.IOException;
-import org.apache.parquet.CorruptDeltaByteArrays;
-import org.apache.parquet.bytes.ByteBufferInputStream;
-import org.apache.parquet.bytes.BytesInput;
-import org.apache.parquet.bytes.BytesUtils;
-import org.apache.parquet.column.ColumnDescriptor;
-import org.apache.parquet.column.Dictionary;
-import org.apache.parquet.column.Encoding;
-import org.apache.parquet.column.ValuesType;
-import org.apache.parquet.column.page.DataPage;
-import org.apache.parquet.column.page.DataPageV1;
-import org.apache.parquet.column.page.DataPageV2;
-import org.apache.parquet.column.values.RequiresPreviousReader;
-import org.apache.parquet.column.values.ValuesReader;
-import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;
-import org.apache.parquet.io.ParquetDecodingException;
-import org.apache.parquet.io.api.Binary;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-abstract class PageIterator<T> implements TripleIterator<T> {
-  private static final Logger LOG = LoggerFactory.getLogger(PageIterator.class);
-
-  @SuppressWarnings("unchecked")
-  static <T> PageIterator<T> newIterator(ColumnDescriptor desc, String writerVersion) {
-    switch (desc.getPrimitiveType().getPrimitiveTypeName()) {
-      case BOOLEAN:
-        return (PageIterator<T>) new PageIterator<Boolean>(desc, writerVersion) {
-          @Override
-          public Boolean next() {
-            return nextBoolean();
-          }
-        };
-      case INT32:
-        return (PageIterator<T>) new PageIterator<Integer>(desc, writerVersion) {
-          @Override
-          public Integer next() {
-            return nextInteger();
-          }
-        };
-      case INT64:
-        return (PageIterator<T>) new PageIterator<Long>(desc, writerVersion) {
-          @Override
-          public Long next() {
-            return nextLong();
-          }
-        };
-      case FLOAT:
-        return (PageIterator<T>) new PageIterator<Float>(desc, writerVersion) {
-          @Override
-          public Float next() {
-            return nextFloat();
-          }
-        };
-      case DOUBLE:
-        return (PageIterator<T>) new PageIterator<Double>(desc, writerVersion) {
-          @Override
-          public Double next() {
-            return nextDouble();
-          }
-        };
-      case FIXED_LEN_BYTE_ARRAY:
-      case BINARY:
-        return (PageIterator<T>) new PageIterator<Binary>(desc, writerVersion) {
-          @Override
-          public Binary next() {
-            return nextBinary();
-          }
-        };
-      default:
-        throw new UnsupportedOperationException("Unsupported primitive type: " +
-            desc.getPrimitiveType().getPrimitiveTypeName());
-    }
-  }
-
-  private final ColumnDescriptor desc;
-  private final String writerVersion;
-
-  // iterator state
-  private boolean hasNext = false;
-  private int triplesRead = 0;
-  private int currentDL = 0;
-  private int currentRL = 0;
-
-  // page bookkeeping
-  private Dictionary dict = null;
-  private DataPage page = null;
-  private int triplesCount = 0;
-  private Encoding valueEncoding = null;
-  private IntIterator definitionLevels = null;
-  private IntIterator repetitionLevels = null;
-  private ValuesReader values = null;
-
-  private PageIterator(ColumnDescriptor desc, String writerVersion) {
-    this.desc = desc;
-    this.writerVersion = writerVersion;
-  }
-
-  public void setPage(DataPage page) {
-    Preconditions.checkNotNull(page, "Cannot read from null page");
-    this.page = page;
-    this.page.accept(new DataPage.Visitor<ValuesReader>() {
-      @Override
-      public ValuesReader visit(DataPageV1 dataPageV1) {
-        initFromPage(dataPageV1);
-        return null;
-      }
-
-      @Override
-      public ValuesReader visit(DataPageV2 dataPageV2) {
-        initFromPage(dataPageV2);
-        return null;
-      }
-    });
-    this.triplesRead = 0;
-    advance();
-  }
-
-  public void setDictionary(Dictionary dictionary) {
-    this.dict = dictionary;
-  }
-
-  public void reset() {
-    this.page = null;
-    this.triplesCount = 0;
-    this.triplesRead = 0;
-    this.definitionLevels = null;
-    this.repetitionLevels = null;
-    this.values = null;
-    this.hasNext = false;
-  }
-
-  public int currentPageCount() {
-    return triplesCount;
-  }
-
-  @Override
-  public boolean hasNext() {
-    return hasNext;
-  }
-
-  @Override
-  public int currentDefinitionLevel() {
-    Preconditions.checkArgument(currentDL >= 0, "Should not read definition, past page end");
-    return currentDL;
-  }
-
-  @Override
-  public int currentRepetitionLevel() {
-//    Preconditions.checkArgument(currentDL >= 0, "Should not read repetition, past page end");
-    return currentRL;
-  }
-
-  @Override
-  public boolean nextBoolean() {
-    advance();
-    try {
-      return values.readBoolean();
-    } catch (RuntimeException e) {
-      throw handleRuntimeException(e);
-    }
-  }
-
-  @Override
-  public int nextInteger() {
-    advance();
-    try {
-      return values.readInteger();
-    } catch (RuntimeException e) {
-      throw handleRuntimeException(e);
-    }
-  }
-
-  @Override
-  public long nextLong() {
-    advance();
-    try {
-      return values.readLong();
-    } catch (RuntimeException e) {
-      throw handleRuntimeException(e);
-    }
-  }
-
-  @Override
-  public float nextFloat() {
-    advance();
-    try {
-      return values.readFloat();
-    } catch (RuntimeException e) {
-      throw handleRuntimeException(e);
-    }
-  }
-
-  @Override
-  public double nextDouble() {
-    advance();
-    try {
-      return values.readDouble();
-    } catch (RuntimeException e) {
-      throw handleRuntimeException(e);
-    }
-  }
-
-  @Override
-  public Binary nextBinary() {
-    advance();
-    try {
-      return values.readBytes();
-    } catch (RuntimeException e) {
-      throw handleRuntimeException(e);
-    }
-  }
-
-  @Override
-  public <V> V nextNull() {
-    advance();
-    // values do not contain nulls
-    return null;
-  }
-
-  private void advance() {
-    if (triplesRead < triplesCount) {
-      this.currentDL = definitionLevels.nextInt();
-      this.currentRL = repetitionLevels.nextInt();
-      this.triplesRead += 1;
-      this.hasNext = true;
-    } else {
-      this.currentDL = -1;
-      this.currentRL = -1;
-      this.hasNext = false;
-    }
-  }
-
-  RuntimeException handleRuntimeException(RuntimeException exception) {
-    if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, valueEncoding) &&
-        exception instanceof ArrayIndexOutOfBoundsException) {
-      // this is probably PARQUET-246, which may happen if reading data with
-      // MR because this can't be detected without reading all footers
-      throw new ParquetDecodingException("Read failure possibly due to " +
-          "PARQUET-246: try setting parquet.split.files to false",
-          new ParquetDecodingException(
-              String.format("Can't read value in column %s at value %d out of %d in current page. " +
-                            "repetition level: %d, definition level: %d",
-                  desc, triplesRead, triplesCount, currentRL, currentDL),
-              exception));
-    }
-    throw new ParquetDecodingException(
-        String.format("Can't read value in column %s at value %d out of %d in current page. " +
-                      "repetition level: %d, definition level: %d",
-            desc, triplesRead, triplesCount, currentRL, currentDL),
-        exception);
-  }
-
-  private void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount) {
-    ValuesReader previousReader = values;
-
-    this.valueEncoding = dataEncoding;
-
-    // TODO: May want to change this so that this class is not dictionary-aware.
-    // For dictionary columns, this class could rely on wrappers to correctly handle dictionaries
-    // This isn't currently possible because RLE must be read by getDictionaryBasedValuesReader
-    if (dataEncoding.usesDictionary()) {
-      if (dict == null) {
-        throw new ParquetDecodingException(
-            "could not read page in col " + desc + " as the dictionary was missing for encoding " + dataEncoding);
-      }
-      this.values = dataEncoding.getDictionaryBasedValuesReader(desc, ValuesType.VALUES, dict);
-    } else {
-      this.values = dataEncoding.getValuesReader(desc, ValuesType.VALUES);
-    }
-
-//    if (dataEncoding.usesDictionary() && converter.hasDictionarySupport()) {
-//      bindToDictionary(dictionary);
-//    } else {
-//      bind(path.getType());
-//    }
-
-    try {
-      values.initFromPage(valueCount, in);
-    } catch (IOException e) {
-      throw new ParquetDecodingException("could not read page in col " + desc, e);
-    }
-
-    if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, dataEncoding) &&
-        previousReader instanceof RequiresPreviousReader) {
-      // previous reader can only be set if reading sequentially
-      ((RequiresPreviousReader) values).setPreviousReader(previousReader);
-    }
-  }
-
-
-  private void initFromPage(DataPageV1 initPage) {
-    this.triplesCount = initPage.getValueCount();
-    ValuesReader rlReader = initPage.getRlEncoding().getValuesReader(desc, ValuesType.REPETITION_LEVEL);
-    ValuesReader dlReader = initPage.getDlEncoding().getValuesReader(desc, ValuesType.DEFINITION_LEVEL);
-    this.repetitionLevels = new ValuesReaderIntIterator(rlReader);
-    this.definitionLevels = new ValuesReaderIntIterator(dlReader);
-    try {
-      BytesInput bytes = initPage.getBytes();
-      LOG.debug("page size {} bytes and {} records", bytes.size(), triplesCount);
-      LOG.debug("reading repetition levels at 0");
-      ByteBufferInputStream in = bytes.toInputStream();
-      rlReader.initFromPage(triplesCount, in);
-      LOG.debug("reading definition levels at {}", in.position());
-      dlReader.initFromPage(triplesCount, in);
-      LOG.debug("reading data at {}", in.position());
-      initDataReader(initPage.getValueEncoding(), in, initPage.getValueCount());
-    } catch (IOException e) {
-      throw new ParquetDecodingException("could not read page " + initPage + " in col " + desc, e);
-    }
-  }
-
-  private void initFromPage(DataPageV2 initPage) {
-    this.triplesCount = initPage.getValueCount();
-    this.repetitionLevels = newRLEIterator(desc.getMaxRepetitionLevel(), initPage.getRepetitionLevels());
-    this.definitionLevels = newRLEIterator(desc.getMaxDefinitionLevel(), initPage.getDefinitionLevels());
-    LOG.debug("page data size {} bytes and {} records", initPage.getData().size(), triplesCount);
-    try {
-      initDataReader(initPage.getDataEncoding(), initPage.getData().toInputStream(), triplesCount);
-    } catch (IOException e) {
-      throw new ParquetDecodingException("could not read page " + initPage + " in col " + desc, e);
-    }
-  }
-
-  private IntIterator newRLEIterator(int maxLevel, BytesInput bytes) {
-    try {
-      if (maxLevel == 0) {
-        return new NullIntIterator();
-      }
-      return new RLEIntIterator(
-          new RunLengthBitPackingHybridDecoder(
-              BytesUtils.getWidthFromMaxInt(maxLevel),
-              bytes.toInputStream()));
-    } catch (IOException e) {
-      throw new ParquetDecodingException("could not read levels in page for col " + desc, e);
-    }
-  }
-
-  abstract static class IntIterator {
-    abstract int nextInt();
-  }
-
-  static class ValuesReaderIntIterator extends IntIterator {
-    private final ValuesReader delegate;
-
-    ValuesReaderIntIterator(ValuesReader delegate) {
-      this.delegate = delegate;
-    }
-
-    @Override
-    int nextInt() {
-      return delegate.readInteger();
-    }
-  }
-
-  static class RLEIntIterator extends IntIterator {
-    private final RunLengthBitPackingHybridDecoder delegate;
-
-    RLEIntIterator(RunLengthBitPackingHybridDecoder delegate) {
-      this.delegate = delegate;
-    }
-
-    @Override
-    int nextInt() {
-      try {
-        return delegate.readInt();
-      } catch (IOException e) {
-        throw new ParquetDecodingException(e);
-      }
-    }
-  }
-
-  private static final class NullIntIterator extends IntIterator {
-    @Override
-    int nextInt() {
-      return 0;
-    }
-  }
-}
diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
index 674243c0..18491101 100644
--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
@@ -43,14 +43,19 @@ import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.Types;
 import org.apache.iceberg.util.BinaryUtil;
 import org.apache.iceberg.util.UnicodeUtil;
+import org.apache.parquet.column.ColumnDescriptor;
+import org.apache.parquet.column.Dictionary;
 import org.apache.parquet.column.Encoding;
 import org.apache.parquet.column.EncodingStats;
+import org.apache.parquet.column.page.DictionaryPage;
+import org.apache.parquet.column.page.PageReader;
 import org.apache.parquet.column.statistics.Statistics;
 import org.apache.parquet.hadoop.ParquetFileReader;
 import org.apache.parquet.hadoop.metadata.BlockMetaData;
 import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;
 import org.apache.parquet.hadoop.metadata.ColumnPath;
 import org.apache.parquet.hadoop.metadata.ParquetMetadata;
+import org.apache.parquet.io.ParquetDecodingException;
 import org.apache.parquet.schema.MessageType;
 
 public class ParquetUtil {
@@ -255,4 +260,19 @@ public class ParquetUtil {
       return true;
     }
   }
+
+  public static Dictionary readDictionary(ColumnDescriptor desc, PageReader pageSource) {
+    DictionaryPage dictionaryPage = pageSource.readDictionaryPage();
+    if (dictionaryPage != null) {
+      try {
+        return dictionaryPage.getEncoding().initDictionary(desc, dictionaryPage);
+//        if (converter.hasDictionarySupport()) {
+//          converter.setDictionary(dictionary);
+//        }
+      } catch (IOException e) {
+        throw new ParquetDecodingException("could not decode the dictionary for " + desc, e);
+      }
+    }
+    return null;
+  }
 }
diff --git a/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java b/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java
index 97cd5912..8552be98 100644
--- a/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java
+++ b/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java
@@ -48,7 +48,7 @@ import static org.apache.iceberg.TableProperties.WRITE_NEW_DATA_LOCATION;
 
 @Fork(1)
 @State(Scope.Benchmark)
-@Warmup(iterations = 3)
+@Warmup(iterations = 1)
 @Measurement(iterations = 5)
 @BenchmarkMode(Mode.SingleShotTime)
 public abstract class IcebergSourceBenchmark {
diff --git a/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedIcebergSourceBenchmark.java b/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedIcebergSourceBenchmark.java
index a47fa813..56900f99 100644
--- a/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedIcebergSourceBenchmark.java
+++ b/spark/src/jmh/java/org/apache/iceberg/spark/source/parquet/vectorized/VectorizedIcebergSourceBenchmark.java
@@ -41,7 +41,7 @@ import static org.apache.iceberg.TableProperties.SPLIT_OPEN_FILE_COST;
  * To run all the the benchmarks that extend this class:
  * <code>
  * ./gradlew :iceberg-spark:jmh -PjmhIncludeRegex=VectorizedRead*Benchmark
- * -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-read-benchmark-result.txt
+   * -PjmhOutputPath=benchmark/iceberg-source-flat-parquet-data-read-benchmark-result.txt
  * </code>
  */
 
@@ -153,69 +153,69 @@ public abstract class VectorizedIcebergSourceBenchmark extends IcebergSourceBenc
     });
   }
 
-  @Benchmark
-  @Threads(1)
-  public void readWithProjectionIcebergVectorized1k() {
-    Map<String, String> tableProperties = Maps.newHashMap();
-    tableProperties.put(SPLIT_OPEN_FILE_COST, Integer.toString(128 * 1024 * 1024));
-    withTableProperties(tableProperties, () -> {
-      String tableLocation = table().location();
-      Dataset<Row> df = spark().read().format("iceberg")
-          .option("iceberg.read.numrecordsperbatch", "1000")
-          .load(tableLocation).select("longCol");
-      materialize(df);
-    });
-  }
-
-  @Benchmark
-  @Threads(1)
-  public void readWithProjectionIcebergVectorized5k() {
-    Map<String, String> tableProperties = Maps.newHashMap();
-    tableProperties.put(SPLIT_OPEN_FILE_COST, Integer.toString(128 * 1024 * 1024));
-    withTableProperties(tableProperties, () -> {
-      String tableLocation = table().location();
-      Dataset<Row> df = spark().read().format("iceberg")
-          .option("iceberg.read.numrecordsperbatch", "5000")
-          .load(tableLocation).select("longCol");
-      materialize(df);
-    });
-  }
-
-  @Benchmark
-  @Threads(1)
-  public void readWithProjectionIcebergVectorized10k() {
-    Map<String, String> tableProperties = Maps.newHashMap();
-    tableProperties.put(SPLIT_OPEN_FILE_COST, Integer.toString(128 * 1024 * 1024));
-    withTableProperties(tableProperties, () -> {
-      String tableLocation = table().location();
-      Dataset<Row> df = spark().read().format("iceberg")
-          .option("iceberg.read.numrecordsperbatch", "10000")
-          .load(tableLocation).select("longCol");
-      materialize(df);
-    });
-  }
-
-  @Benchmark
-  @Threads(1)
-  public void readWithProjectionFileSourceVectorized() {
-    Map<String, String> conf = Maps.newHashMap();
-    conf.put(SQLConf.PARQUET_VECTORIZED_READER_ENABLED().key(), "true");
-    conf.put(SQLConf.FILES_OPEN_COST_IN_BYTES().key(), Integer.toString(128 * 1024 * 1024));
-    withSQLConf(conf, () -> {
-      Dataset<Row> df = spark().read().parquet(dataLocation()).select("longCol");
-      materialize(df);
-    });
-  }
-
-  @Benchmark
-  @Threads(1)
-  public void readWithProjectionFileSourceNonVectorized() {
-    Map<String, String> conf = Maps.newHashMap();
-    conf.put(SQLConf.PARQUET_VECTORIZED_READER_ENABLED().key(), "false");
-    conf.put(SQLConf.FILES_OPEN_COST_IN_BYTES().key(), Integer.toString(128 * 1024 * 1024));
-    withSQLConf(conf, () -> {
-      Dataset<Row> df = spark().read().parquet(dataLocation()).select("longCol");
-      materialize(df);
-    });
-  }
+//  @Benchmark
+//  @Threads(1)
+//  public void readWithProjectionIcebergVectorized1k() {
+//    Map<String, String> tableProperties = Maps.newHashMap();
+//    tableProperties.put(SPLIT_OPEN_FILE_COST, Integer.toString(128 * 1024 * 1024));
+//    withTableProperties(tableProperties, () -> {
+//      String tableLocation = table().location();
+//      Dataset<Row> df = spark().read().format("iceberg")
+//          .option("iceberg.read.numrecordsperbatch", "1000")
+//          .load(tableLocation).select("longCol");
+//      materialize(df);
+//    });
+//  }
+//
+//  @Benchmark
+//  @Threads(1)
+//  public void readWithProjectionIcebergVectorized5k() {
+//    Map<String, String> tableProperties = Maps.newHashMap();
+//    tableProperties.put(SPLIT_OPEN_FILE_COST, Integer.toString(128 * 1024 * 1024));
+//    withTableProperties(tableProperties, () -> {
+//      String tableLocation = table().location();
+//      Dataset<Row> df = spark().read().format("iceberg")
+//          .option("iceberg.read.numrecordsperbatch", "5000")
+//          .load(tableLocation).select("longCol");
+//      materialize(df);
+//    });
+//  }
+//
+//  @Benchmark
+//  @Threads(1)
+//  public void readWithProjectionIcebergVectorized10k() {
+//    Map<String, String> tableProperties = Maps.newHashMap();
+//    tableProperties.put(SPLIT_OPEN_FILE_COST, Integer.toString(128 * 1024 * 1024));
+//    withTableProperties(tableProperties, () -> {
+//      String tableLocation = table().location();
+//      Dataset<Row> df = spark().read().format("iceberg")
+//          .option("iceberg.read.numrecordsperbatch", "10000")
+//          .load(tableLocation).select("longCol");
+//      materialize(df);
+//    });
+//  }
+//
+//  @Benchmark
+//  @Threads(1)
+//  public void readWithProjectionFileSourceVectorized() {
+//    Map<String, String> conf = Maps.newHashMap();
+//    conf.put(SQLConf.PARQUET_VECTORIZED_READER_ENABLED().key(), "true");
+//    conf.put(SQLConf.FILES_OPEN_COST_IN_BYTES().key(), Integer.toString(128 * 1024 * 1024));
+//    withSQLConf(conf, () -> {
+//      Dataset<Row> df = spark().read().parquet(dataLocation()).select("longCol");
+//      materialize(df);
+//    });
+//  }
+//
+//  @Benchmark
+//  @Threads(1)
+//  public void readWithProjectionFileSourceNonVectorized() {
+//    Map<String, String> conf = Maps.newHashMap();
+//    conf.put(SQLConf.PARQUET_VECTORIZED_READER_ENABLED().key(), "false");
+//    conf.put(SQLConf.FILES_OPEN_COST_IN_BYTES().key(), Integer.toString(128 * 1024 * 1024));
+//    withSQLConf(conf, () -> {
+//      Dataset<Row> df = spark().read().parquet(dataLocation()).select("longCol");
+//      materialize(df);
+//    });
+//  }
 }
diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java
index 47b2a92e..795a8fb2 100644
--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java
+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java
@@ -74,7 +74,7 @@ public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {
     int numRows = 0;
     for (int i = 0; i < readers.length; i += 1) {
       VectorHolder holder = readers[i].read(numValsToRead);
-      FieldVector vector = holder.getVector();
+      FieldVector vector = holder.vector();
       if (vector == null) {
         arrowColumnVectors[i] = new NullValuesColumnVector(batchSize);
       } else {
diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java
index 09583d34..a5204a0e 100644
--- a/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java
+++ b/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java
@@ -36,9 +36,7 @@ import org.apache.arrow.vector.VarBinaryVector;
 import org.apache.arrow.vector.complex.ListVector;
 import org.apache.arrow.vector.complex.StructVector;
 import org.apache.arrow.vector.holders.NullableVarCharHolder;
-import org.apache.iceberg.arrow.vectorized.IcebergDecimalArrowVector;
-import org.apache.iceberg.arrow.vectorized.IcebergVarBinaryArrowVector;
-import org.apache.iceberg.arrow.vectorized.IcebergVarcharArrowVector;
+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;
 import org.apache.iceberg.arrow.vectorized.NullabilityHolder;
 import org.apache.iceberg.arrow.vectorized.VectorHolder;
 import org.apache.iceberg.spark.arrow.ArrowUtils;
@@ -56,27 +54,27 @@ import org.apache.spark.sql.vectorized.ColumnarMap;
 import org.apache.spark.unsafe.types.UTF8String;
 
 /**
- * Implementation of Spark's {@link ColumnVector} interface. The main purpose of this class is to prevent the expensive
- * nullability checks made by Spark's {@link ArrowColumnVector} implementation by delegating those calls to the
- * Iceberg's {@link NullabilityHolder}.
+ * Implementation of Spark's {@link ColumnVector} interface. The code for this
+ * class is heavily inspired from Spark's {@link ArrowColumnVector}
+ * The main difference is in how nullability checks are made in this
+ * class by relying on {@link NullabilityHolder} instead of the validity vector
+ * in the Arrow vector.
  */
 
 public class IcebergArrowColumnVector extends ColumnVector {
 
   private final ArrowVectorAccessor accessor;
   private final NullabilityHolder nullabilityHolder;
-  private final ColumnDescriptor columnDescriptor;
   private final Dictionary dictionary;
   private final boolean isVectorDictEncoded;
   private ArrowColumnVector[] childColumns;
 
   public IcebergArrowColumnVector(VectorHolder holder) {
-    super(ArrowUtils.instance().fromArrowField(holder.getVector().getField()));
-    this.nullabilityHolder = holder.getNullabilityHolder();
-    this.columnDescriptor = holder.getDescriptor();
-    this.dictionary = holder.getDictionary();
+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));
+    this.nullabilityHolder = holder.nullabilityHolder();
+    this.dictionary = holder.dictionary();
     this.isVectorDictEncoded = holder.isDictionaryEncoded();
-    this.accessor = getVectorAccessor(columnDescriptor, holder.getVector());
+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());
   }
 
   // public for testing purposes only
@@ -108,7 +106,7 @@ public class IcebergArrowColumnVector extends ColumnVector {
 
   @Override
   public boolean isNullAt(int rowId) {
-    return nullabilityHolder.isNullAt(rowId);
+    return nullabilityHolder.isNullAt(rowId) == 1;
   }
 
   @Override
@@ -197,9 +195,8 @@ public class IcebergArrowColumnVector extends ColumnVector {
       this.vector = vector;
     }
 
-    // TODO: should be final after removing ArrayAccessor workaround
-    boolean isNullAt(int rowId) {
-      return nullabilityHolder.isNullAt(rowId);
+    final boolean isNullAt(int rowId) {
+      return nullabilityHolder.isNullAt(rowId) == 1;
     }
 
     final void close() {
@@ -313,10 +310,6 @@ public class IcebergArrowColumnVector extends ColumnVector {
             return new DictionaryIntAccessor((IntVector) vector);
           case FLOAT:
             return new DictionaryFloatAccessor((IntVector) vector);
-          //        case BOOLEAN:
-          //          this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);
-          //          ((BitVector) vec).allocateNew(batchSize);
-          //          return UNKNOWN_WIDTH;
           case INT64:
             return new DictionaryLongAccessor((IntVector) vector);
           case DOUBLE:
@@ -340,12 +333,12 @@ public class IcebergArrowColumnVector extends ColumnVector {
         return new FloatAccessor((Float4Vector) vector);
       } else if (vector instanceof Float8Vector) {
         return new DoubleAccessor((Float8Vector) vector);
-      } else if (vector instanceof IcebergDecimalArrowVector) {
-        return new DecimalAccessor((IcebergDecimalArrowVector) vector);
-      } else if (vector instanceof IcebergVarcharArrowVector) {
-        return new StringAccessor((IcebergVarcharArrowVector) vector);
-      } else if (vector instanceof IcebergVarBinaryArrowVector) {
-        return new BinaryAccessor((IcebergVarBinaryArrowVector) vector);
+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {
+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);
+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {
+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);
+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {
+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);
       } else if (vector instanceof DateDayVector) {
         return new DateAccessor((DateDayVector) vector);
       } else if (vector instanceof TimeStampMicroTZVector) {
@@ -533,9 +526,9 @@ public class IcebergArrowColumnVector extends ColumnVector {
 
   private class DecimalAccessor extends ArrowVectorAccessor {
 
-    private final IcebergDecimalArrowVector vector;
+    private final IcebergArrowVectors.DecimalArrowVector vector;
 
-    DecimalAccessor(IcebergDecimalArrowVector vector) {
+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {
       super(vector);
       this.vector = vector;
     }
@@ -551,10 +544,10 @@ public class IcebergArrowColumnVector extends ColumnVector {
 
   private class StringAccessor extends ArrowVectorAccessor {
 
-    private final IcebergVarcharArrowVector vector;
+    private final IcebergArrowVectors.VarcharArrowVector vector;
     private final NullableVarCharHolder stringResult = new NullableVarCharHolder();
 
-    StringAccessor(IcebergVarcharArrowVector vector) {
+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {
       super(vector);
       this.vector = vector;
     }
@@ -691,16 +684,6 @@ public class IcebergArrowColumnVector extends ColumnVector {
       this.arrayData = new ArrowColumnVector(vector.getDataVector());
     }
 
-    @Override
-    final boolean isNullAt(int rowId) {
-      // TODO: Workaround if vector has all non-null values, see ARROW-1948
-      if (vector.getValueCount() > 0 && vector.getValidityBuffer().capacity() == 0) {
-        return false;
-      } else {
-        return super.isNullAt(rowId);
-      }
-    }
-
     @Override
     final ColumnarArray getArray(int rowId) {
       ArrowBuf offsets = vector.getOffsetBuffer();
diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
index 2b8b2994..db2afc09 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java
@@ -230,9 +230,12 @@ public class TestHelpers {
     }
   }
 
-  public static void assertArrowVectors(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {
+  public static void assertArrowVectors(Types.StructType struct, List<Record> expected,
+                                        ColumnarBatch batch, int batchNum) {
     List<Types.NestedField> fields = struct.fields();
+    int rowNum = 0;
     for (int r = 0; r < batch.numRows(); r++) {
+      rowNum++;
       Record expRec = expected.get(r);
       InternalRow actualRow = batch.getRow(r);
       for (int i = 0; i < fields.size(); i += 1) {
@@ -243,11 +246,14 @@ public class TestHelpers {
         Type fieldType = fields.get(i).type();
         Object expectedValue = expRec.get(i);
         if (actualRow.isNullAt(i)) {
-          Assert.assertTrue("Expect null at " + r, expectedValue == null);
-          Assert.assertTrue("Expected the value to be set as null in the arrow vector", arrowVector.isNull(r));
+          Assert.assertTrue("Expected the value to be set as null at row " + rowNum +
+              " with batch position " + r + " for batch number " + batchNum + " for field " +
+              fields.get(i) + " in the arrow vector", arrowVector.isNull(r));
         } else {
           Object actualValue = actualRow.get(i, convert(fieldType));
-          Assert.assertFalse("Expected the value to be set as non-null in the arrow vector", arrowVector.isNull(r));
+          Assert.assertFalse("Expected the value to be set as non-null at row " + rowNum +
+                  " with batch position " + r + " for field " + fields.get(i) + " in the arrow vector ",
+              arrowVector.isNull(r));
           assertEqualsUnsafe(fieldType, expectedValue, actualValue);
         }
       }
diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetVectorizedReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetVectorizedReader.java
index 5350fef4..773f6166 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetVectorizedReader.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetVectorizedReader.java
@@ -74,12 +74,14 @@ public class TestSparkParquetVectorizedReader extends AvroDataTest {
   void assertRecordsMatch(Schema schema, List<GenericData.Record> expected, File testFile) throws IOException {
     try (CloseableIterable<ColumnarBatch> batchReader = Parquet.read(Files.localInput(testFile))
         .project(schema)
+        .reuseContainers()
         .createBatchedReaderFunc(type -> VectorizedSparkParquetReaders.buildReader(schema, schema, type, 10000))
         .build()) {
 
       Iterator<ColumnarBatch> batches = batchReader.iterator();
       int numRowsRead = 0;
       int numExpectedRead = 0;
+      int batchNum = 0;
       while (batches.hasNext()) {
 
         ColumnarBatch batch = batches.next();
@@ -89,8 +91,9 @@ public class TestSparkParquetVectorizedReader extends AvroDataTest {
         for (int i = numExpectedRead; i < numExpectedRead + batch.numRows(); i++) {
           expectedBatch.add(expected.get(i));
         }
-        assertArrowVectors(schema.asStruct(), expectedBatch, batch);
+        assertArrowVectors(schema.asStruct(), expectedBatch, batch, batchNum);
         numExpectedRead += batch.numRows();
+        batchNum++;
       }
       Assert.assertEquals(expected.size(), numRowsRead);
     }
diff --git a/versions.lock b/versions.lock
index 1debafb6..7f3ca394 100644
--- a/versions.lock
+++ b/versions.lock
@@ -66,7 +66,7 @@ io.dropwizard.metrics:metrics-graphite:3.1.5 (1 constraints: 1a0dc936)
 io.dropwizard.metrics:metrics-json:3.1.5 (1 constraints: 1a0dc936)
 io.dropwizard.metrics:metrics-jvm:3.1.5 (1 constraints: 1a0dc936)
 io.netty:netty:3.9.9.Final (9 constraints: 9eb0396d)
-io.netty:netty-all:4.1.17.Final (3 constraints: d2312526)
+io.netty:netty-all:4.1.42.Final (3 constraints: ce319b25)
 io.netty:netty-buffer:4.1.27.Final (1 constraints: 4a0fee77)
 javax.activation:activation:1.1.1 (1 constraints: 140dbb36)
 javax.annotation:javax.annotation-api:1.2 (2 constraints: 2d21193d)
@@ -98,7 +98,7 @@ org.apache.ant:ant:1.9.1 (3 constraints: a721ed14)
 org.apache.ant:ant-launcher:1.9.1 (1 constraints: 69082485)
 org.apache.arrow:arrow-format:0.14.1 (1 constraints: 240df421)
 org.apache.arrow:arrow-memory:0.14.1 (1 constraints: 240df421)
-org.apache.arrow:arrow-vector:0.14.1 (2 constraints: 2012a545)
+org.apache.arrow:arrow-vector:0.14.1 (2 constraints: 2512b245)
 org.apache.avro:avro:1.8.2 (4 constraints: 3d2eebf3)
 org.apache.avro:avro-ipc:1.8.2 (1 constraints: f90b5bf4)
 org.apache.avro:avro-mapred:1.8.2 (2 constraints: 3a1a4787)
@@ -158,18 +158,18 @@ org.apache.parquet:parquet-format:2.4.0 (3 constraints: e72a97ca)
 org.apache.parquet:parquet-hadoop:1.10.1 (2 constraints: de1ac5b3)
 org.apache.parquet:parquet-jackson:1.10.1 (1 constraints: b70ee763)
 org.apache.pig:pig:0.14.0 (1 constraints: 37052f3b)
-org.apache.spark:spark-avro_2.11:2.4.4 (1 constraints: 0c050536)
-org.apache.spark:spark-catalyst_2.11:2.4.4 (1 constraints: c20cc327)
-org.apache.spark:spark-core_2.11:2.4.4 (3 constraints: b528ed99)
-org.apache.spark:spark-hive_2.11:2.4.4 (1 constraints: 0c050536)
-org.apache.spark:spark-kvstore_2.11:2.4.4 (1 constraints: 1b0dcc36)
-org.apache.spark:spark-launcher_2.11:2.4.4 (1 constraints: 1b0dcc36)
-org.apache.spark:spark-network-common_2.11:2.4.4 (2 constraints: b01eeee2)
-org.apache.spark:spark-network-shuffle_2.11:2.4.4 (1 constraints: 1b0dcc36)
-org.apache.spark:spark-sketch_2.11:2.4.4 (2 constraints: 981bd4f5)
-org.apache.spark:spark-sql_2.11:2.4.4 (1 constraints: 1e0d0037)
-org.apache.spark:spark-tags_2.11:2.4.4 (8 constraints: 036fa69d)
-org.apache.spark:spark-unsafe_2.11:2.4.4 (2 constraints: f11bc213)
+org.apache.spark:spark-avro_2.11:2.4.5-SNAPSHOT (1 constraints: aa07f66f)
+org.apache.spark:spark-catalyst_2.11:2.4.5-SNAPSHOT (1 constraints: 600f1aa7)
+org.apache.spark:spark-core_2.11:2.4.5-SNAPSHOT (3 constraints: 8f309f32)
+org.apache.spark:spark-hive_2.11:2.4.5-SNAPSHOT (1 constraints: aa07f66f)
+org.apache.spark:spark-kvstore_2.11:2.4.5-SNAPSHOT (1 constraints: b90f44b9)
+org.apache.spark:spark-launcher_2.11:2.4.5-SNAPSHOT (1 constraints: b90f44b9)
+org.apache.spark:spark-network-common_2.11:2.4.5-SNAPSHOT (2 constraints: ec23ee25)
+org.apache.spark:spark-network-shuffle_2.11:2.4.5-SNAPSHOT (1 constraints: b90f44b9)
+org.apache.spark:spark-sketch_2.11:2.4.5-SNAPSHOT (2 constraints: d420300d)
+org.apache.spark:spark-sql_2.11:2.4.5-SNAPSHOT (1 constraints: bc0f93b9)
+org.apache.spark:spark-tags_2.11:2.4.5-SNAPSHOT (8 constraints: f3834268)
+org.apache.spark:spark-unsafe_2.11:2.4.5-SNAPSHOT (2 constraints: 2d21ce30)
 org.apache.thrift:libfb303:0.9.3 (3 constraints: 6725fac0)
 org.apache.thrift:libthrift:0.9.3 (5 constraints: 71415452)
 org.apache.xbean:xbean-asm6-shaded:4.8 (2 constraints: 2419a30f)
diff --git a/versions.props b/versions.props
index 6f8723cc..89e656fd 100644
--- a/versions.props
+++ b/versions.props
@@ -6,8 +6,8 @@ org.apache.hadoop:* = 2.7.3
 org.apache.hive:hive-standalone-metastore = 1.2.1
 org.apache.orc:orc-core = 1.5.6
 org.apache.parquet:parquet-avro = 1.10.1
-org.apache.spark:spark-hive_2.11 = 2.4.4
-org.apache.spark:spark-avro_2.11 = 2.4.4
+org.apache.spark:spark-hive_2.11 = 2.4.5-SNAPSHOT
+org.apache.spark:spark-avro_2.11 = 2.4.5-SNAPSHOT
 org.apache.pig:pig = 0.14.0
 org.apache.commons:commons-lang3 = 3.9
 com.fasterxml.jackson.*:* = 2.10.0
